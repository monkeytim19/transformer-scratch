{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from online\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncodingTokenizer:\n",
    "  def __init__(self,word_split = 'Ġ'):\n",
    "    self.vocab={}\n",
    "    self.inverse_vocab={}\n",
    "    self.tokens=[]\n",
    "    self.bpe_pairs={}\n",
    "    self.word_split = word_split\n",
    "\n",
    "  def set_vocabulary(self, word_split, special_tokens):\n",
    "    # Step 1: Setting the unique characters\n",
    "    unique_characters=[chr(i) for i in range(256)]\n",
    "\n",
    "    # Step 2: Word split characters\n",
    "    if word_split not in unique_characters:\n",
    "      unique_characters.append(self.word_split)\n",
    "\n",
    "    # Step 3: Special tokens\n",
    "    if special_tokens:\n",
    "      unique_characters.extend(self.special_tokens)\n",
    "\n",
    "    return unique_characters\n",
    "\n",
    "  def get_max_freq_pair(self, tokens):\n",
    "    # pair_counts={}\n",
    "    pairs=[]\n",
    "    # Step 1: Gettkng all the token pairs-> (token[i],token[i+1])\n",
    "    for index in range(len(tokens)-1):\n",
    "      pairs.append((tokens[index],tokens[index+1]))\n",
    "\n",
    "    # Step 2: Getting the count of occurences of each of token pairs\n",
    "    pairs_counts=Counter(pairs)\n",
    "\n",
    "    # Step 3: Get the token pair whose count is the highest\n",
    "    max_pair=max(pairs_counts.items(),key=lambda x: x[1])[0]\n",
    "    return max_pair\n",
    "\n",
    "  def merge_tokens(self, tokens, max_pair, new_pair_id):\n",
    "    # In the tokens, check the presence of occurence of max_pair and if exists then replace max_pair with new_pair_id\n",
    "    # Eg: tokens=[87,76,44,25,38,44,25,19], max_pair=[44,25],  new_pair_id=123\n",
    "    # Output: [87,76,123,38,123,19]\n",
    "    new_tokens=[]\n",
    "    i=0\n",
    "    while i<=len(tokens)-1:\n",
    "      if i==len(tokens)-1:\n",
    "        new_tokens.append(tokens[i])\n",
    "        break\n",
    "\n",
    "      elif (tokens[i],tokens[i+1])==max_pair:\n",
    "        new_tokens.append(new_pair_id)\n",
    "        i+=2\n",
    "\n",
    "      else:\n",
    "        new_tokens.append(tokens[i])\n",
    "        i+=1\n",
    "    return new_tokens\n",
    "\n",
    "  def train(self, text, vocab_size, special_tokens):\n",
    "    if vocab_size <= 258:\n",
    "      raise ValueError('Please enter a vocab size greater than 258 since this defines the basic set of characters')\n",
    "    self.special_tokens = special_tokens\n",
    "\n",
    "    # Setting the vocabulary\n",
    "    vocab = self.set_vocabulary(self.word_split, self.special_tokens)\n",
    "    for index,character in enumerate(vocab):\n",
    "      self.vocab[index]=character\n",
    "      self.inverse_vocab[character]=index\n",
    "\n",
    "    # Transforming the text\n",
    "    ## Step 1: Replacing all thw white-space character\n",
    "    processed_text=[]\n",
    "    for index,char in enumerate(text):\n",
    "      if index != 0 and char == ' ':\n",
    "        processed_text.append(self.word_split)\n",
    "      if char != ' ':\n",
    "        processed_text.append(char)\n",
    "    processed_text=\"\".join(processed_text)\n",
    "\n",
    "    ## Step 2: Getting the numerical form of token\n",
    "    self.tokens = []\n",
    "    for char in processed_text:\n",
    "      self.tokens.append(self.inverse_vocab[char])\n",
    "\n",
    "    ## Step 3: BPE-algorithm\n",
    "    vocab_length = len(self.vocab)\n",
    "    for i in range(vocab_length,vocab_size):\n",
    "      max_pair=self.get_max_freq_pair(self.tokens)\n",
    "      if max_pair is None:\n",
    "        break\n",
    "      self.bpe_pairs[max_pair]=i\n",
    "      self.tokens=self.merge_tokens(self.tokens, max_pair, i)\n",
    "\n",
    "    ## Step 4: Update vocab with BPE\n",
    "    for pair,new_index in self.bpe_pairs.items():\n",
    "      merged_token=self.vocab[pair[0]]+self.vocab[pair[1]]\n",
    "      self.vocab[new_index]=merged_token\n",
    "      self.inverse_vocab[merged_token]=new_index\n",
    "\n",
    "  def encode(self, text):\n",
    "    # Step 1: Basically tokens are split into words. Replace all the occurences of \"\\n\" to \" <NEWLINE> \". This is to avoid splitting issues.\n",
    "    tokens_split=text.replace('\\n',' <NEWLINE> ').split()\n",
    "    tokens=[]\n",
    "    for i in tokens_split:\n",
    "      if i=='<NEWLINE>':\n",
    "        tokens.append('\\n')\n",
    "      else:\n",
    "        tokens.append(i)\n",
    "\n",
    "    # Step 2: Cleaning of tokens\n",
    "    ## Eg: 'This is a ball' will be tokenized as ['The','Ġis','Ġa', 'Ġball']\n",
    "    # Ensures that all the tokens in a line other than the first one will be prefixed with \"Ġ\" to show the word boundaries\n",
    "    tokens_cleaned=[]\n",
    "    for index,token in enumerate(tokens):\n",
    "      if index>0 and not token.startswith('\\n'):\n",
    "        tokens_cleaned.append(self.word_split+token)\n",
    "      else:\n",
    "        tokens_cleaned.append(token)\n",
    "\n",
    "    # Step 3: Getting the corresponding token IDs from the cleaned tokens\n",
    "    ## Checks whether tokens exist in the vocabulary. If not, then perform BPE tokenization of the token\n",
    "    token_ids=[]\n",
    "    for token in tokens_cleaned:\n",
    "      if token in self.inverse_vocab.keys():\n",
    "        token_ids.append(self.inverse_vocab[token])\n",
    "      else:\n",
    "        token_ids.extend(self.tokenize_using_bpe(token))\n",
    "    return token_ids\n",
    "\n",
    "  def tokenize_using_bpe(self, token):\n",
    "    # Step 1: Mapping the tokens to their IDs from the vocabulary\n",
    "    token_ids=[]\n",
    "    for char in token:\n",
    "      if char in self.inverse_vocab.keys():\n",
    "        token_ids.append(self.inverse_vocab[char])\n",
    "      else:\n",
    "        token_ids.append(None)\n",
    "\n",
    "    # Step 2: Check whether token does not exist in Vocabulary- In that case stop\n",
    "    if None in token_ids:\n",
    "      token_dict=dict(zip(token_ids,token))\n",
    "      missing_characters=[]\n",
    "      for id,ch in token_dict.items():\n",
    "        if id is None:\n",
    "          missing_characters.append(ch)\n",
    "      raise ValueError(f\"No token IDs found for the characters:{missing_characters}\")\n",
    "\n",
    "    # Step 3: Now merging\n",
    "    can_merge=True\n",
    "    while can_merge and len(token_ids)>1:\n",
    "      can_merge=False\n",
    "      i=0\n",
    "      new_tokens=[]\n",
    "      \"\"\"\n",
    "      Check whether the token pair is part of bpe_pairs occured during training,\n",
    "      If yes, index = index + 2, else index = index + 1.\n",
    "      This iteration occurs until there exists no merging exists for all the tokens in token_ids.\n",
    "      No merging exists means that there are no more possible keys to merge in bpe_pairs.\n",
    "      \"\"\"\n",
    "      while i<len(token_ids)-1:\n",
    "        pair=(token_ids[i],token_ids[i+1])\n",
    "        if pair in self.bpe_pairs.keys():\n",
    "          pair_id=self.bpe_pairs[pair]\n",
    "          new_tokens.append(pair_id)\n",
    "          i+=2\n",
    "          can_merge=True\n",
    "        else:\n",
    "          new_tokens.append(token_ids[i])\n",
    "          i+=1\n",
    "      if i<len(token_ids):\n",
    "        new_tokens.append(token_ids[i])\n",
    "      token_ids=new_tokens\n",
    "\n",
    "    return token_ids\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    # Step 1: Check whether there are non-existing token IDs\n",
    "    non_existing_ids=[]\n",
    "    for id in token_ids:\n",
    "      if id not in self.vocab.keys():\n",
    "        non_existing_ids.append(id)\n",
    "    if len(non_existing_ids)>0:\n",
    "      raise ValueError(f\"No token found for the token IDs:{non_existing_ids}\")\n",
    "\n",
    "    # Step 2: Decoding- Check whether text corresponding to token ID starts with word_split-symbol('Ġ'). If yes replace word_split-symbol with \" \" else just append the text to string\n",
    "    final=\"\"\n",
    "    for id in token_ids:\n",
    "      text=self.vocab[id]\n",
    "      if text.startswith(self.word_split):\n",
    "        final+=\" \"+text[1:]\n",
    "      else:\n",
    "        final+=\"\"+text\n",
    "\n",
    "    return final\n",
    "\n",
    "  def save_bpe_vocab_and_merges(self, vocab_path, bpe_path):\n",
    "    with open(vocab_path,'w',encoding='utf-8') as f:\n",
    "      json.dump(self.vocab,f,ensure_ascii=False, indent=2)\n",
    "    with open(bpe_path,'w',encoding='utf-8') as f:\n",
    "      json.dump([{'pair':list(pair),'id':id } for pair,id in self.bpe_pairs.items()],f,\n",
    "                ensure_ascii=False, indent=2)\n",
    "\n",
    "  def load_bpe_vocab_and_merges(self, vocab_path, bpe_path):\n",
    "    with open(vocab_path,'r',encoding='utf-8') as f:\n",
    "      loaded_vocab=json.load(f)\n",
    "      self.vocab = {int(id):token for id,token in loaded_vocab.items()}\n",
    "      self.inverse_vocab={token:int(id) for id,token in self.vocab.items()}\n",
    "    with open(bpe_path,'r',encoding='utf-8') as f:\n",
    "      bpe=json.load(f)\n",
    "      for merge in bpe:\n",
    "        self.bpe_pairs[tuple(merge['pair'])]=merge['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BytePairEncodingTokenizer()\n",
    "tokenizer.train(text, vocab_size=259, special_tokens=(\"<|endoftext|>\", \"@\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\x00',\n",
       " 1: '\\x01',\n",
       " 2: '\\x02',\n",
       " 3: '\\x03',\n",
       " 4: '\\x04',\n",
       " 5: '\\x05',\n",
       " 6: '\\x06',\n",
       " 7: '\\x07',\n",
       " 8: '\\x08',\n",
       " 9: '\\t',\n",
       " 10: '\\n',\n",
       " 11: '\\x0b',\n",
       " 12: '\\x0c',\n",
       " 13: '\\r',\n",
       " 14: '\\x0e',\n",
       " 15: '\\x0f',\n",
       " 16: '\\x10',\n",
       " 17: '\\x11',\n",
       " 18: '\\x12',\n",
       " 19: '\\x13',\n",
       " 20: '\\x14',\n",
       " 21: '\\x15',\n",
       " 22: '\\x16',\n",
       " 23: '\\x17',\n",
       " 24: '\\x18',\n",
       " 25: '\\x19',\n",
       " 26: '\\x1a',\n",
       " 27: '\\x1b',\n",
       " 28: '\\x1c',\n",
       " 29: '\\x1d',\n",
       " 30: '\\x1e',\n",
       " 31: '\\x1f',\n",
       " 32: ' ',\n",
       " 33: '!',\n",
       " 34: '\"',\n",
       " 35: '#',\n",
       " 36: '$',\n",
       " 37: '%',\n",
       " 38: '&',\n",
       " 39: \"'\",\n",
       " 40: '(',\n",
       " 41: ')',\n",
       " 42: '*',\n",
       " 43: '+',\n",
       " 44: ',',\n",
       " 45: '-',\n",
       " 46: '.',\n",
       " 47: '/',\n",
       " 48: '0',\n",
       " 49: '1',\n",
       " 50: '2',\n",
       " 51: '3',\n",
       " 52: '4',\n",
       " 53: '5',\n",
       " 54: '6',\n",
       " 55: '7',\n",
       " 56: '8',\n",
       " 57: '9',\n",
       " 58: ':',\n",
       " 59: ';',\n",
       " 60: '<',\n",
       " 61: '=',\n",
       " 62: '>',\n",
       " 63: '?',\n",
       " 64: '@',\n",
       " 65: 'A',\n",
       " 66: 'B',\n",
       " 67: 'C',\n",
       " 68: 'D',\n",
       " 69: 'E',\n",
       " 70: 'F',\n",
       " 71: 'G',\n",
       " 72: 'H',\n",
       " 73: 'I',\n",
       " 74: 'J',\n",
       " 75: 'K',\n",
       " 76: 'L',\n",
       " 77: 'M',\n",
       " 78: 'N',\n",
       " 79: 'O',\n",
       " 80: 'P',\n",
       " 81: 'Q',\n",
       " 82: 'R',\n",
       " 83: 'S',\n",
       " 84: 'T',\n",
       " 85: 'U',\n",
       " 86: 'V',\n",
       " 87: 'W',\n",
       " 88: 'X',\n",
       " 89: 'Y',\n",
       " 90: 'Z',\n",
       " 91: '[',\n",
       " 92: '\\\\',\n",
       " 93: ']',\n",
       " 94: '^',\n",
       " 95: '_',\n",
       " 96: '`',\n",
       " 97: 'a',\n",
       " 98: 'b',\n",
       " 99: 'c',\n",
       " 100: 'd',\n",
       " 101: 'e',\n",
       " 102: 'f',\n",
       " 103: 'g',\n",
       " 104: 'h',\n",
       " 105: 'i',\n",
       " 106: 'j',\n",
       " 107: 'k',\n",
       " 108: 'l',\n",
       " 109: 'm',\n",
       " 110: 'n',\n",
       " 111: 'o',\n",
       " 112: 'p',\n",
       " 113: 'q',\n",
       " 114: 'r',\n",
       " 115: 's',\n",
       " 116: 't',\n",
       " 117: 'u',\n",
       " 118: 'v',\n",
       " 119: 'w',\n",
       " 120: 'x',\n",
       " 121: 'y',\n",
       " 122: 'z',\n",
       " 123: '{',\n",
       " 124: '|',\n",
       " 125: '}',\n",
       " 126: '~',\n",
       " 127: '\\x7f',\n",
       " 128: '\\x80',\n",
       " 129: '\\x81',\n",
       " 130: '\\x82',\n",
       " 131: '\\x83',\n",
       " 132: '\\x84',\n",
       " 133: '\\x85',\n",
       " 134: '\\x86',\n",
       " 135: '\\x87',\n",
       " 136: '\\x88',\n",
       " 137: '\\x89',\n",
       " 138: '\\x8a',\n",
       " 139: '\\x8b',\n",
       " 140: '\\x8c',\n",
       " 141: '\\x8d',\n",
       " 142: '\\x8e',\n",
       " 143: '\\x8f',\n",
       " 144: '\\x90',\n",
       " 145: '\\x91',\n",
       " 146: '\\x92',\n",
       " 147: '\\x93',\n",
       " 148: '\\x94',\n",
       " 149: '\\x95',\n",
       " 150: '\\x96',\n",
       " 151: '\\x97',\n",
       " 152: '\\x98',\n",
       " 153: '\\x99',\n",
       " 154: '\\x9a',\n",
       " 155: '\\x9b',\n",
       " 156: '\\x9c',\n",
       " 157: '\\x9d',\n",
       " 158: '\\x9e',\n",
       " 159: '\\x9f',\n",
       " 160: '\\xa0',\n",
       " 161: '¡',\n",
       " 162: '¢',\n",
       " 163: '£',\n",
       " 164: '¤',\n",
       " 165: '¥',\n",
       " 166: '¦',\n",
       " 167: '§',\n",
       " 168: '¨',\n",
       " 169: '©',\n",
       " 170: 'ª',\n",
       " 171: '«',\n",
       " 172: '¬',\n",
       " 173: '\\xad',\n",
       " 174: '®',\n",
       " 175: '¯',\n",
       " 176: '°',\n",
       " 177: '±',\n",
       " 178: '²',\n",
       " 179: '³',\n",
       " 180: '´',\n",
       " 181: 'µ',\n",
       " 182: '¶',\n",
       " 183: '·',\n",
       " 184: '¸',\n",
       " 185: '¹',\n",
       " 186: 'º',\n",
       " 187: '»',\n",
       " 188: '¼',\n",
       " 189: '½',\n",
       " 190: '¾',\n",
       " 191: '¿',\n",
       " 192: 'À',\n",
       " 193: 'Á',\n",
       " 194: 'Â',\n",
       " 195: 'Ã',\n",
       " 196: 'Ä',\n",
       " 197: 'Å',\n",
       " 198: 'Æ',\n",
       " 199: 'Ç',\n",
       " 200: 'È',\n",
       " 201: 'É',\n",
       " 202: 'Ê',\n",
       " 203: 'Ë',\n",
       " 204: 'Ì',\n",
       " 205: 'Í',\n",
       " 206: 'Î',\n",
       " 207: 'Ï',\n",
       " 208: 'Ð',\n",
       " 209: 'Ñ',\n",
       " 210: 'Ò',\n",
       " 211: 'Ó',\n",
       " 212: 'Ô',\n",
       " 213: 'Õ',\n",
       " 214: 'Ö',\n",
       " 215: '×',\n",
       " 216: 'Ø',\n",
       " 217: 'Ù',\n",
       " 218: 'Ú',\n",
       " 219: 'Û',\n",
       " 220: 'Ü',\n",
       " 221: 'Ý',\n",
       " 222: 'Þ',\n",
       " 223: 'ß',\n",
       " 224: 'à',\n",
       " 225: 'á',\n",
       " 226: 'â',\n",
       " 227: 'ã',\n",
       " 228: 'ä',\n",
       " 229: 'å',\n",
       " 230: 'æ',\n",
       " 231: 'ç',\n",
       " 232: 'è',\n",
       " 233: 'é',\n",
       " 234: 'ê',\n",
       " 235: 'ë',\n",
       " 236: 'ì',\n",
       " 237: 'í',\n",
       " 238: 'î',\n",
       " 239: 'ï',\n",
       " 240: 'ð',\n",
       " 241: 'ñ',\n",
       " 242: 'ò',\n",
       " 243: 'ó',\n",
       " 244: 'ô',\n",
       " 245: 'õ',\n",
       " 246: 'ö',\n",
       " 247: '÷',\n",
       " 248: 'ø',\n",
       " 249: 'ù',\n",
       " 250: 'ú',\n",
       " 251: 'û',\n",
       " 252: 'ü',\n",
       " 253: 'ý',\n",
       " 254: 'þ',\n",
       " 255: 'ÿ',\n",
       " 256: 'Ġ',\n",
       " 257: '<|endoftext|>',\n",
       " 258: '@'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[87, 104, 111, 256, 97, 114, 101, 256, 121, 111, 117, 63]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tokenizer on sample text\n",
    "sample = \"Who are you?\"\n",
    "token_ids = tokenizer.encode(sample)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who are you?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode token_ids back to original text\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 87, 104, 111, 256,  97, 114, 101, 256, 121, 111, 117,  63])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sequence as tensor\n",
    "token_long_tensor = torch.LongTensor(token_ids)\n",
    "token_long_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    super().__init__()\n",
    "    self.embedding_dim=embedding_dim\n",
    "    self.embedding=nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                embedding_dim=embedding_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return np.sqrt(self.embedding_dim)*self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(num_embeddings=len(tokenizer.vocab), embedding_dim=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9856, -2.3575,  2.0718, -2.0467,  2.8428,  0.4432,  0.7987,  1.2985],\n",
       "        [ 0.9228, -1.9629,  4.6919,  5.3943,  1.2417, -3.3060, -0.6195, -3.4327],\n",
       "        [ 1.8080, -3.7972, -1.0322,  1.3885, -1.5262, -1.6721, -6.0577,  4.9831],\n",
       "        [ 2.0784,  2.4770,  2.5817,  0.4005, -2.8100,  4.3268,  4.2190, -1.9063],\n",
       "        [-3.9400,  5.2402, -0.0523,  2.4181, -0.0347, -0.0770, -2.3273, -1.4514],\n",
       "        [ 2.1183,  3.2613,  2.8089,  2.0002,  3.0198,  2.5460,  0.7063,  0.7371],\n",
       "        [ 0.8294,  1.8098, -2.6556, -0.1901,  1.9772, -0.2603,  0.5946,  2.2008],\n",
       "        [ 2.0784,  2.4770,  2.5817,  0.4005, -2.8100,  4.3268,  4.2190, -1.9063],\n",
       "        [-4.3962,  4.5073, -2.3406,  1.9577, -3.4309, -3.2198,  3.0411, -1.2057],\n",
       "        [ 1.8080, -3.7972, -1.0322,  1.3885, -1.5262, -1.6721, -6.0577,  4.9831],\n",
       "        [-4.3329, -0.2546,  1.4490, -0.9599, -0.3142,  2.9750, -1.6957,  1.4664],\n",
       "        [ 2.8154, -0.4107, -4.5981,  1.1376, -1.4648, -2.8645,  5.1517, -1.8352]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_tokens = embedding(token_long_tensor)\n",
    "embedded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 111, Embedding tensor([ 1.8080, -3.7972, -1.0322,  1.3885, -1.5262, -1.6721, -6.0577,  4.9831],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Token {token_ids[2]}, Embedding {embedded_tokens[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 111, Embedding tensor([ 1.8080, -3.7972, -1.0322,  1.3885, -1.5262, -1.6721, -6.0577,  4.9831],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Token {token_ids[9]}, Embedding {embedded_tokens[9]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
