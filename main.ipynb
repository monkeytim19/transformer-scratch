{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from online\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "We use Byte Pair Encoding as the tokenizer for our purpose. This encompasses the following steps\n",
    "\n",
    "1. We initialize the vocabulary with the character vocabulary where each word is represented as a sequence of characters. \n",
    "2. The symbol pairs are iteratively counted and replace the most frequent pair say ('A','B') with 'AB' which is a new symbol.\n",
    "3. Every merge operation introduces a new symbol which represents a character n-gram. \n",
    "4. Frequent character n-grams (or whole words) are eventually merged into a single symbol, thus BPE requires no shortlist.\n",
    "5. The final symbol vocabulary size is equal to the size of the initial vocabulary, plus the number of merge operations– the latter is the only hyperparameter of the algorithm.\n",
    "\n",
    "For efficiency, we do not consider pairs that cross word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncodingTokenizer:\n",
    "  def __init__(self,word_split = 'Ġ'):\n",
    "    self.vocab={}\n",
    "    self.inverse_vocab={}\n",
    "    self.tokens=[]\n",
    "    self.bpe_pairs={}\n",
    "    self.word_split = word_split\n",
    "\n",
    "  def set_vocabulary(self, word_split, special_tokens):\n",
    "    # Step 1: Setting the unique characters\n",
    "    unique_characters=[chr(i) for i in range(256)]\n",
    "\n",
    "    # Step 2: Word split characters\n",
    "    if word_split not in unique_characters:\n",
    "      unique_characters.append(self.word_split)\n",
    "\n",
    "    # Step 3: Special tokens\n",
    "    if special_tokens:\n",
    "      unique_characters.extend(self.special_tokens)\n",
    "\n",
    "    return unique_characters\n",
    "\n",
    "  def get_max_freq_pair(self, tokens):\n",
    "    # pair_counts={}\n",
    "    pairs=[]\n",
    "    # Step 1: Gettkng all the token pairs-> (token[i],token[i+1])\n",
    "    for index in range(len(tokens)-1):\n",
    "      pairs.append((tokens[index],tokens[index+1]))\n",
    "\n",
    "    # Step 2: Getting the count of occurences of each of token pairs\n",
    "    pairs_counts=Counter(pairs)\n",
    "\n",
    "    # Step 3: Get the token pair whose count is the highest\n",
    "    max_pair=max(pairs_counts.items(),key=lambda x: x[1])[0]\n",
    "    return max_pair\n",
    "\n",
    "  def merge_tokens(self, tokens, max_pair, new_pair_id):\n",
    "    # In the tokens, check the presence of occurence of max_pair and if exists then replace max_pair with new_pair_id\n",
    "    # Eg: tokens=[87,76,44,25,38,44,25,19], max_pair=[44,25],  new_pair_id=123\n",
    "    # Output: [87,76,123,38,123,19]\n",
    "    new_tokens=[]\n",
    "    i=0\n",
    "    while i<=len(tokens)-1:\n",
    "      if i==len(tokens)-1:\n",
    "        new_tokens.append(tokens[i])\n",
    "        break\n",
    "\n",
    "      elif (tokens[i],tokens[i+1])==max_pair:\n",
    "        new_tokens.append(new_pair_id)\n",
    "        i+=2\n",
    "\n",
    "      else:\n",
    "        new_tokens.append(tokens[i])\n",
    "        i+=1\n",
    "    return new_tokens\n",
    "\n",
    "  def train(self, text, vocab_size, special_tokens):\n",
    "    if vocab_size <= 258:\n",
    "      raise ValueError('Please enter a vocab size greater than 258 since this defines the basic set of characters')\n",
    "    self.special_tokens = special_tokens\n",
    "\n",
    "    # Setting the vocabulary\n",
    "    vocab = self.set_vocabulary(self.word_split, self.special_tokens)\n",
    "    for index,character in enumerate(vocab):\n",
    "      self.vocab[index]=character\n",
    "      self.inverse_vocab[character]=index\n",
    "\n",
    "    # Transforming the text\n",
    "    ## Step 1: Replacing all thw white-space character\n",
    "    processed_text=[]\n",
    "    for index,char in enumerate(text):\n",
    "      if index != 0 and char == ' ':\n",
    "        processed_text.append(self.word_split)\n",
    "      if char != ' ':\n",
    "        processed_text.append(char)\n",
    "    processed_text=\"\".join(processed_text)\n",
    "\n",
    "    ## Step 2: Getting the numerical form of token\n",
    "    self.tokens = []\n",
    "    for char in processed_text:\n",
    "      self.tokens.append(self.inverse_vocab[char])\n",
    "\n",
    "    ## Step 3: BPE-algorithm\n",
    "    vocab_length = len(self.vocab)\n",
    "    for i in range(vocab_length,vocab_size):\n",
    "      max_pair=self.get_max_freq_pair(self.tokens)\n",
    "      if max_pair is None:\n",
    "        break\n",
    "      self.bpe_pairs[max_pair]=i\n",
    "      self.tokens=self.merge_tokens(self.tokens, max_pair, i)\n",
    "\n",
    "    ## Step 4: Update vocab with BPE\n",
    "    for pair,new_index in self.bpe_pairs.items():\n",
    "      merged_token=self.vocab[pair[0]]+self.vocab[pair[1]]\n",
    "      self.vocab[new_index]=merged_token\n",
    "      self.inverse_vocab[merged_token]=new_index\n",
    "\n",
    "  def encode(self, text):\n",
    "    # Step 1: Basically tokens are split into words. Replace all the occurences of \"\\n\" to \" <NEWLINE> \". This is to avoid splitting issues.\n",
    "    tokens_split=text.replace('\\n',' <NEWLINE> ').split()\n",
    "    tokens=[]\n",
    "    for i in tokens_split:\n",
    "      if i=='<NEWLINE>':\n",
    "        tokens.append('\\n')\n",
    "      else:\n",
    "        tokens.append(i)\n",
    "\n",
    "    # Step 2: Cleaning of tokens\n",
    "    ## Eg: 'This is a ball' will be tokenized as ['The','Ġis','Ġa', 'Ġball']\n",
    "    # Ensures that all the tokens in a line other than the first one will be prefixed with \"Ġ\" to show the word boundaries\n",
    "    tokens_cleaned=[]\n",
    "    for index,token in enumerate(tokens):\n",
    "      if index>0 and not token.startswith('\\n'):\n",
    "        tokens_cleaned.append(self.word_split+token)\n",
    "      else:\n",
    "        tokens_cleaned.append(token)\n",
    "\n",
    "    # Step 3: Getting the corresponding token IDs from the cleaned tokens\n",
    "    ## Checks whether tokens exist in the vocabulary. If not, then perform BPE tokenization of the token\n",
    "    token_ids=[]\n",
    "    for token in tokens_cleaned:\n",
    "      if token in self.inverse_vocab.keys():\n",
    "        token_ids.append(self.inverse_vocab[token])\n",
    "      else:\n",
    "        token_ids.extend(self.tokenize_using_bpe(token))\n",
    "    return token_ids\n",
    "\n",
    "  def tokenize_using_bpe(self, token):\n",
    "    # Step 1: Mapping the tokens to their IDs from the vocabulary\n",
    "    token_ids=[]\n",
    "    for char in token:\n",
    "      if char in self.inverse_vocab.keys():\n",
    "        token_ids.append(self.inverse_vocab[char])\n",
    "      else:\n",
    "        token_ids.append(None)\n",
    "\n",
    "    # Step 2: Check whether token does not exist in Vocabulary- In that case stop\n",
    "    if None in token_ids:\n",
    "      token_dict=dict(zip(token_ids,token))\n",
    "      missing_characters=[]\n",
    "      for id,ch in token_dict.items():\n",
    "        if id is None:\n",
    "          missing_characters.append(ch)\n",
    "      raise ValueError(f\"No token IDs found for the characters:{missing_characters}\")\n",
    "\n",
    "    # Step 3: Now merging\n",
    "    can_merge=True\n",
    "    while can_merge and len(token_ids)>1:\n",
    "      can_merge=False\n",
    "      i=0\n",
    "      new_tokens=[]\n",
    "      \"\"\"\n",
    "      Check whether the token pair is part of bpe_pairs occured during training,\n",
    "      If yes, index = index + 2, else index = index + 1.\n",
    "      This iteration occurs until there exists no merging exists for all the tokens in token_ids.\n",
    "      No merging exists means that there are no more possible keys to merge in bpe_pairs.\n",
    "      \"\"\"\n",
    "      while i<len(token_ids)-1:\n",
    "        pair=(token_ids[i],token_ids[i+1])\n",
    "        if pair in self.bpe_pairs.keys():\n",
    "          pair_id=self.bpe_pairs[pair]\n",
    "          new_tokens.append(pair_id)\n",
    "          i+=2\n",
    "          can_merge=True\n",
    "        else:\n",
    "          new_tokens.append(token_ids[i])\n",
    "          i+=1\n",
    "      if i<len(token_ids):\n",
    "        new_tokens.append(token_ids[i])\n",
    "      token_ids=new_tokens\n",
    "\n",
    "    return token_ids\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    # Step 1: Check whether there are non-existing token IDs\n",
    "    non_existing_ids=[]\n",
    "    for id in token_ids:\n",
    "      if id not in self.vocab.keys():\n",
    "        non_existing_ids.append(id)\n",
    "    if len(non_existing_ids)>0:\n",
    "      raise ValueError(f\"No token found for the token IDs:{non_existing_ids}\")\n",
    "\n",
    "    # Step 2: Decoding- Check whether text corresponding to token ID starts with word_split-symbol('Ġ'). If yes replace word_split-symbol with \" \" else just append the text to string\n",
    "    final=\"\"\n",
    "    for id in token_ids:\n",
    "      text=self.vocab[id]\n",
    "      if text.startswith(self.word_split):\n",
    "        final+=\" \"+text[1:]\n",
    "      else:\n",
    "        final+=\"\"+text\n",
    "\n",
    "    return final\n",
    "\n",
    "  def save_bpe_vocab_and_merges(self, vocab_path, bpe_path):\n",
    "    with open(vocab_path,'w',encoding='utf-8') as f:\n",
    "      json.dump(self.vocab,f,ensure_ascii=False, indent=2)\n",
    "    with open(bpe_path,'w',encoding='utf-8') as f:\n",
    "      json.dump([{'pair':list(pair),'id':id } for pair,id in self.bpe_pairs.items()],f,\n",
    "                ensure_ascii=False, indent=2)\n",
    "\n",
    "  def load_bpe_vocab_and_merges(self, vocab_path, bpe_path):\n",
    "    with open(vocab_path,'r',encoding='utf-8') as f:\n",
    "      loaded_vocab=json.load(f)\n",
    "      self.vocab = {int(id):token for id,token in loaded_vocab.items()}\n",
    "      self.inverse_vocab={token:int(id) for id,token in self.vocab.items()}\n",
    "    with open(bpe_path,'r',encoding='utf-8') as f:\n",
    "      bpe=json.load(f)\n",
    "      for merge in bpe:\n",
    "        self.bpe_pairs[tuple(merge['pair'])]=merge['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BytePairEncodingTokenizer()\n",
    "tokenizer.train(text, vocab_size=259, special_tokens=(\"<|endoftext|>\", \"@\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\x00',\n",
       " 1: '\\x01',\n",
       " 2: '\\x02',\n",
       " 3: '\\x03',\n",
       " 4: '\\x04',\n",
       " 5: '\\x05',\n",
       " 6: '\\x06',\n",
       " 7: '\\x07',\n",
       " 8: '\\x08',\n",
       " 9: '\\t',\n",
       " 10: '\\n',\n",
       " 11: '\\x0b',\n",
       " 12: '\\x0c',\n",
       " 13: '\\r',\n",
       " 14: '\\x0e',\n",
       " 15: '\\x0f',\n",
       " 16: '\\x10',\n",
       " 17: '\\x11',\n",
       " 18: '\\x12',\n",
       " 19: '\\x13',\n",
       " 20: '\\x14',\n",
       " 21: '\\x15',\n",
       " 22: '\\x16',\n",
       " 23: '\\x17',\n",
       " 24: '\\x18',\n",
       " 25: '\\x19',\n",
       " 26: '\\x1a',\n",
       " 27: '\\x1b',\n",
       " 28: '\\x1c',\n",
       " 29: '\\x1d',\n",
       " 30: '\\x1e',\n",
       " 31: '\\x1f',\n",
       " 32: ' ',\n",
       " 33: '!',\n",
       " 34: '\"',\n",
       " 35: '#',\n",
       " 36: '$',\n",
       " 37: '%',\n",
       " 38: '&',\n",
       " 39: \"'\",\n",
       " 40: '(',\n",
       " 41: ')',\n",
       " 42: '*',\n",
       " 43: '+',\n",
       " 44: ',',\n",
       " 45: '-',\n",
       " 46: '.',\n",
       " 47: '/',\n",
       " 48: '0',\n",
       " 49: '1',\n",
       " 50: '2',\n",
       " 51: '3',\n",
       " 52: '4',\n",
       " 53: '5',\n",
       " 54: '6',\n",
       " 55: '7',\n",
       " 56: '8',\n",
       " 57: '9',\n",
       " 58: ':',\n",
       " 59: ';',\n",
       " 60: '<',\n",
       " 61: '=',\n",
       " 62: '>',\n",
       " 63: '?',\n",
       " 64: '@',\n",
       " 65: 'A',\n",
       " 66: 'B',\n",
       " 67: 'C',\n",
       " 68: 'D',\n",
       " 69: 'E',\n",
       " 70: 'F',\n",
       " 71: 'G',\n",
       " 72: 'H',\n",
       " 73: 'I',\n",
       " 74: 'J',\n",
       " 75: 'K',\n",
       " 76: 'L',\n",
       " 77: 'M',\n",
       " 78: 'N',\n",
       " 79: 'O',\n",
       " 80: 'P',\n",
       " 81: 'Q',\n",
       " 82: 'R',\n",
       " 83: 'S',\n",
       " 84: 'T',\n",
       " 85: 'U',\n",
       " 86: 'V',\n",
       " 87: 'W',\n",
       " 88: 'X',\n",
       " 89: 'Y',\n",
       " 90: 'Z',\n",
       " 91: '[',\n",
       " 92: '\\\\',\n",
       " 93: ']',\n",
       " 94: '^',\n",
       " 95: '_',\n",
       " 96: '`',\n",
       " 97: 'a',\n",
       " 98: 'b',\n",
       " 99: 'c',\n",
       " 100: 'd',\n",
       " 101: 'e',\n",
       " 102: 'f',\n",
       " 103: 'g',\n",
       " 104: 'h',\n",
       " 105: 'i',\n",
       " 106: 'j',\n",
       " 107: 'k',\n",
       " 108: 'l',\n",
       " 109: 'm',\n",
       " 110: 'n',\n",
       " 111: 'o',\n",
       " 112: 'p',\n",
       " 113: 'q',\n",
       " 114: 'r',\n",
       " 115: 's',\n",
       " 116: 't',\n",
       " 117: 'u',\n",
       " 118: 'v',\n",
       " 119: 'w',\n",
       " 120: 'x',\n",
       " 121: 'y',\n",
       " 122: 'z',\n",
       " 123: '{',\n",
       " 124: '|',\n",
       " 125: '}',\n",
       " 126: '~',\n",
       " 127: '\\x7f',\n",
       " 128: '\\x80',\n",
       " 129: '\\x81',\n",
       " 130: '\\x82',\n",
       " 131: '\\x83',\n",
       " 132: '\\x84',\n",
       " 133: '\\x85',\n",
       " 134: '\\x86',\n",
       " 135: '\\x87',\n",
       " 136: '\\x88',\n",
       " 137: '\\x89',\n",
       " 138: '\\x8a',\n",
       " 139: '\\x8b',\n",
       " 140: '\\x8c',\n",
       " 141: '\\x8d',\n",
       " 142: '\\x8e',\n",
       " 143: '\\x8f',\n",
       " 144: '\\x90',\n",
       " 145: '\\x91',\n",
       " 146: '\\x92',\n",
       " 147: '\\x93',\n",
       " 148: '\\x94',\n",
       " 149: '\\x95',\n",
       " 150: '\\x96',\n",
       " 151: '\\x97',\n",
       " 152: '\\x98',\n",
       " 153: '\\x99',\n",
       " 154: '\\x9a',\n",
       " 155: '\\x9b',\n",
       " 156: '\\x9c',\n",
       " 157: '\\x9d',\n",
       " 158: '\\x9e',\n",
       " 159: '\\x9f',\n",
       " 160: '\\xa0',\n",
       " 161: '¡',\n",
       " 162: '¢',\n",
       " 163: '£',\n",
       " 164: '¤',\n",
       " 165: '¥',\n",
       " 166: '¦',\n",
       " 167: '§',\n",
       " 168: '¨',\n",
       " 169: '©',\n",
       " 170: 'ª',\n",
       " 171: '«',\n",
       " 172: '¬',\n",
       " 173: '\\xad',\n",
       " 174: '®',\n",
       " 175: '¯',\n",
       " 176: '°',\n",
       " 177: '±',\n",
       " 178: '²',\n",
       " 179: '³',\n",
       " 180: '´',\n",
       " 181: 'µ',\n",
       " 182: '¶',\n",
       " 183: '·',\n",
       " 184: '¸',\n",
       " 185: '¹',\n",
       " 186: 'º',\n",
       " 187: '»',\n",
       " 188: '¼',\n",
       " 189: '½',\n",
       " 190: '¾',\n",
       " 191: '¿',\n",
       " 192: 'À',\n",
       " 193: 'Á',\n",
       " 194: 'Â',\n",
       " 195: 'Ã',\n",
       " 196: 'Ä',\n",
       " 197: 'Å',\n",
       " 198: 'Æ',\n",
       " 199: 'Ç',\n",
       " 200: 'È',\n",
       " 201: 'É',\n",
       " 202: 'Ê',\n",
       " 203: 'Ë',\n",
       " 204: 'Ì',\n",
       " 205: 'Í',\n",
       " 206: 'Î',\n",
       " 207: 'Ï',\n",
       " 208: 'Ð',\n",
       " 209: 'Ñ',\n",
       " 210: 'Ò',\n",
       " 211: 'Ó',\n",
       " 212: 'Ô',\n",
       " 213: 'Õ',\n",
       " 214: 'Ö',\n",
       " 215: '×',\n",
       " 216: 'Ø',\n",
       " 217: 'Ù',\n",
       " 218: 'Ú',\n",
       " 219: 'Û',\n",
       " 220: 'Ü',\n",
       " 221: 'Ý',\n",
       " 222: 'Þ',\n",
       " 223: 'ß',\n",
       " 224: 'à',\n",
       " 225: 'á',\n",
       " 226: 'â',\n",
       " 227: 'ã',\n",
       " 228: 'ä',\n",
       " 229: 'å',\n",
       " 230: 'æ',\n",
       " 231: 'ç',\n",
       " 232: 'è',\n",
       " 233: 'é',\n",
       " 234: 'ê',\n",
       " 235: 'ë',\n",
       " 236: 'ì',\n",
       " 237: 'í',\n",
       " 238: 'î',\n",
       " 239: 'ï',\n",
       " 240: 'ð',\n",
       " 241: 'ñ',\n",
       " 242: 'ò',\n",
       " 243: 'ó',\n",
       " 244: 'ô',\n",
       " 245: 'õ',\n",
       " 246: 'ö',\n",
       " 247: '÷',\n",
       " 248: 'ø',\n",
       " 249: 'ù',\n",
       " 250: 'ú',\n",
       " 251: 'û',\n",
       " 252: 'ü',\n",
       " 253: 'ý',\n",
       " 254: 'þ',\n",
       " 255: 'ÿ',\n",
       " 256: 'Ġ',\n",
       " 257: '<|endoftext|>',\n",
       " 258: '@'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[87, 104, 111, 256, 97, 114, 101, 256, 121, 111, 117, 63]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tokenizer on sample text\n",
    "sample = \"Who are you?\"\n",
    "token_ids = tokenizer.encode(sample)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who are you?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode token_ids back to original text\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "\n",
    "We use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 87, 104, 111, 256,  97, 114, 101, 256, 121, 111, 117,  63])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sequence as tensor\n",
    "token_long_tensor = torch.LongTensor(token_ids)\n",
    "token_long_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    super().__init__()\n",
    "    self.embedding_dim=embedding_dim\n",
    "    self.embedding=nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                embedding_dim=embedding_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return np.sqrt(self.embedding_dim)*self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(num_embeddings=len(tokenizer.vocab), embedding_dim=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.5416e-01, -1.7670e+00,  2.1068e+00, -4.1173e+00,  7.5981e-01,\n",
       "          1.1021e+00, -6.3376e-01,  3.3582e+00],\n",
       "        [-2.2502e+00, -1.2011e+00,  6.6511e+00, -4.3043e-01, -2.1113e+00,\n",
       "          1.7882e+00,  3.8869e-01, -4.7831e+00],\n",
       "        [ 4.4844e+00,  5.2736e+00, -1.9191e+00,  1.1307e+00, -1.1659e+00,\n",
       "          5.6371e+00,  3.6841e+00, -2.3914e+00],\n",
       "        [-1.1385e+00, -1.6680e+00,  4.2019e+00,  3.7996e+00,  5.1161e+00,\n",
       "          1.2670e+00,  1.3824e+00,  1.1164e+00],\n",
       "        [ 9.2625e-01,  2.0720e-01,  6.7946e+00, -5.2076e+00, -4.7721e+00,\n",
       "          5.5989e+00, -2.8884e+00,  8.4984e-01],\n",
       "        [ 2.8368e+00,  7.9613e-01,  1.7057e+00,  4.0691e-01, -1.4522e+00,\n",
       "          4.5691e-01,  7.3491e-01, -2.1884e+00],\n",
       "        [-8.0211e-01,  2.3011e+00,  3.7422e+00,  4.1184e+00,  2.2650e+00,\n",
       "         -2.6388e+00, -2.1120e+00, -3.6979e+00],\n",
       "        [-1.1385e+00, -1.6680e+00,  4.2019e+00,  3.7996e+00,  5.1161e+00,\n",
       "          1.2670e+00,  1.3824e+00,  1.1164e+00],\n",
       "        [-7.0914e-01, -5.2571e-01, -1.4681e+00, -8.5145e-01,  5.7384e-01,\n",
       "         -7.4228e-01,  9.0239e-01,  5.6356e+00],\n",
       "        [ 4.4844e+00,  5.2736e+00, -1.9191e+00,  1.1307e+00, -1.1659e+00,\n",
       "          5.6371e+00,  3.6841e+00, -2.3914e+00],\n",
       "        [ 4.4534e-03,  8.4092e-01, -1.1213e+00,  2.3563e-01, -2.3174e+00,\n",
       "          1.0403e+00,  2.3357e+00, -1.2638e+00],\n",
       "        [ 3.1448e+00, -7.0055e+00, -1.9042e+00, -2.9022e+00, -4.0722e+00,\n",
       "          1.7911e+00,  2.8101e+00,  1.8928e+00]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_tokens = embedding(token_long_tensor)\n",
    "embedded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 111, Embedding tensor([ 4.4844,  5.2736, -1.9191,  1.1307, -1.1659,  5.6371,  3.6841, -2.3914],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Token {token_ids[2]}, Embedding {embedded_tokens[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 111, Embedding tensor([ 4.4844,  5.2736, -1.9191,  1.1307, -1.1659,  5.6371,  3.6841, -2.3914],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Token {token_ids[9]}, Embedding {embedded_tokens[9]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "We use positional encoding to make the model aware of the order of sequence. Attention mechanism does not use the concept of position. To solve this we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. \n",
    "\n",
    "We use sin and cosine functions of different frequencies\n",
    "- $PE(pos,2i) = sin(pos/10000^{2i/dmodel})$\n",
    "- $PE(pos,2i+1) = cos(pos/10000^{2i/dmodel})$\n",
    "\n",
    "Each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to 10000 · $2\\pi$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Sinusoidal Positional Encoding.\n",
    "    \n",
    "    wavelength: factor to determine the wavelength in the sinusoidal function.\n",
    "    \"\"\"\n",
    "    def __init__(self, wavelength=10000.):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.wavelength = wavelength\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Given a (... x seq_len x embedding_dim) tensor, returns a (seq_len x embedding_dim) tensor.\"\"\"\n",
    "        seq_len, embedding_dim = x.shape[-2], x.shape[-1]\n",
    "        pe = torch.zeros((seq_len, embedding_dim))\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        factor = torch.exp(-math.log(self.wavelength) * torch.arange(0, embedding_dim, 2) / embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * factor)\n",
    "        pe[:, 1::2] = torch.cos(position * factor)\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "          9.9920e-01,  4.0000e-03,  9.9999e-01],\n",
       "        [-9.5892e-01,  2.8366e-01,  4.7943e-01,  8.7758e-01,  4.9979e-02,\n",
       "          9.9875e-01,  5.0000e-03,  9.9999e-01],\n",
       "        [-2.7942e-01,  9.6017e-01,  5.6464e-01,  8.2534e-01,  5.9964e-02,\n",
       "          9.9820e-01,  6.0000e-03,  9.9998e-01],\n",
       "        [ 6.5699e-01,  7.5390e-01,  6.4422e-01,  7.6484e-01,  6.9943e-02,\n",
       "          9.9755e-01,  6.9999e-03,  9.9998e-01],\n",
       "        [ 9.8936e-01, -1.4550e-01,  7.1736e-01,  6.9671e-01,  7.9915e-02,\n",
       "          9.9680e-01,  7.9999e-03,  9.9997e-01],\n",
       "        [ 4.1212e-01, -9.1113e-01,  7.8333e-01,  6.2161e-01,  8.9879e-02,\n",
       "          9.9595e-01,  8.9999e-03,  9.9996e-01],\n",
       "        [-5.4402e-01, -8.3907e-01,  8.4147e-01,  5.4030e-01,  9.9833e-02,\n",
       "          9.9500e-01,  9.9998e-03,  9.9995e-01],\n",
       "        [-9.9999e-01,  4.4257e-03,  8.9121e-01,  4.5360e-01,  1.0978e-01,\n",
       "          9.9396e-01,  1.1000e-02,  9.9994e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding()\n",
    "pe(embedded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.5416e-01, -7.6701e-01,  2.1068e+00, -3.1173e+00,  7.5981e-01,\n",
       "          2.1021e+00, -6.3376e-01,  4.3582e+00],\n",
       "        [-1.4087e+00, -6.6075e-01,  6.7509e+00,  5.6457e-01, -2.1013e+00,\n",
       "          2.7882e+00,  3.8969e-01, -3.7831e+00],\n",
       "        [ 5.3937e+00,  4.8574e+00, -1.7205e+00,  2.1107e+00, -1.1459e+00,\n",
       "          6.6369e+00,  3.6861e+00, -1.3914e+00],\n",
       "        [-9.9740e-01, -2.6580e+00,  4.4974e+00,  4.7549e+00,  5.1461e+00,\n",
       "          2.2666e+00,  1.3854e+00,  2.1164e+00],\n",
       "        [ 1.6945e-01, -4.4645e-01,  7.1840e+00, -4.2865e+00, -4.7321e+00,\n",
       "          6.5981e+00, -2.8844e+00,  1.8498e+00],\n",
       "        [ 1.8779e+00,  1.0798e+00,  2.1851e+00,  1.2845e+00, -1.4022e+00,\n",
       "          1.4557e+00,  7.3991e-01, -1.1885e+00],\n",
       "        [-1.0815e+00,  3.2613e+00,  4.3068e+00,  4.9437e+00,  2.3250e+00,\n",
       "         -1.6406e+00, -2.1060e+00, -2.6979e+00],\n",
       "        [-4.8153e-01, -9.1414e-01,  4.8461e+00,  4.5644e+00,  5.1860e+00,\n",
       "          2.2646e+00,  1.3894e+00,  2.1164e+00],\n",
       "        [ 2.8022e-01, -6.7121e-01, -7.5079e-01, -1.5474e-01,  6.5376e-01,\n",
       "          2.5452e-01,  9.1039e-01,  6.6356e+00],\n",
       "        [ 4.8966e+00,  4.3625e+00, -1.1358e+00,  1.7523e+00, -1.0760e+00,\n",
       "          6.6331e+00,  3.6931e+00, -1.3915e+00],\n",
       "        [-5.3957e-01,  1.8523e-03, -2.7980e-01,  7.7593e-01, -2.2175e+00,\n",
       "          2.0353e+00,  2.3457e+00, -2.6382e-01],\n",
       "        [ 2.1448e+00, -7.0011e+00, -1.0130e+00, -2.4486e+00, -3.9624e+00,\n",
       "          2.7851e+00,  2.8211e+00,  2.8928e+00]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_embeddings=pe(embedded_tokens)+embedded_tokens\n",
    "encoded_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
