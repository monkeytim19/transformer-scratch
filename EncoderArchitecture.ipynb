{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a947564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from attention import *\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3abde13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\theju\\\\transformer-scratch'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea765406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (1.26.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from fsspec[http]>=2021.11.1->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (0.8.1)\n",
      "Requirement already satisfied: packaging in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: filelock in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "751e15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b43112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03cfc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0c5c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ffa80e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pybind11>=2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31809d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 11.0/15.8 MB 57.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 55.3 MB/s  0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.2\n",
      "    Uninstalling numpy-2.3.2:\n",
      "      Successfully uninstalled numpy-2.3.2\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\theju\\anaconda3\\envs\\mlp\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\theju\\anaconda3\\envs\\mlp\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"numpy<2\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7c7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9bc7af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /mnt/c/Users/theju/transformer-scratch/roneneldan/TinyStories/TinyStories.py or any data file in the same directory. Couldn't find 'roneneldan/TinyStories' on the Hugging Face Hub either: FileNotFoundError: Dataset 'roneneldan/TinyStories' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroneneldan/TinyStories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforce_redownload\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.9/site-packages/datasets/load.py:1759\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1755\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1756\u001b[0m )\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1759\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.9/site-packages/datasets/load.py:1496\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1495\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1496\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.9/site-packages/datasets/load.py:1214\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m-> 1214\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1215\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1216\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /mnt/c/Users/theju/transformer-scratch/roneneldan/TinyStories/TinyStories.py or any data file in the same directory. Couldn't find 'roneneldan/TinyStories' on the Hugging Face Hub either: FileNotFoundError: Dataset 'roneneldan/TinyStories' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\", trust_remote_code=True, download_mode=\"force_redownload\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c1aed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (2.10.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (1.26.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.9.5)\n",
      "Requirement already satisfied: certifi in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (2024.7.4)\n",
      "Requirement already satisfied: hstspreload in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: sniffio in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from httpcore==0.9.*->httpx<1.0.0->datasets) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from httpcore==0.9.*->httpx<1.0.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx<1.0.0->datasets) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx<1.0.0->datasets) (3.0.0)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.5.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: anyio in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (3.5.0)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/thejussk/miniconda3/envs/mlp/lib/python3.9/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.7)\n",
      "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-1.0.1-py3-none-any.whl (503 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.8/503.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-21.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typer-slim, shellingham, pyarrow, hf-xet, h11, httpcore, httpx, huggingface-hub, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 16.1.0\n",
      "    Uninstalling pyarrow-16.1.0:\n",
      "      Successfully uninstalled pyarrow-16.1.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.9.0\n",
      "    Uninstalling h11-0.9.0:\n",
      "      Successfully uninstalled h11-0.9.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.9.1\n",
      "    Uninstalling httpcore-0.9.1:\n",
      "      Successfully uninstalled httpcore-0.9.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.13.3\n",
      "    Uninstalling httpx-0.13.3:\n",
      "      Successfully uninstalled httpx-0.13.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.8.1\n",
      "    Uninstalling huggingface-hub-0.8.1:\n",
      "      Successfully uninstalled huggingface-hub-0.8.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.10.1\n",
      "    Uninstalling datasets-2.10.1:\n",
      "      Successfully uninstalled datasets-2.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "allennlp 2.10.1 requires transformers<4.21,>=4.1, but you have transformers 4.21.0 which is incompatible.\n",
      "cached-path 1.1.6 requires huggingface-hub<0.11.0,>=0.8.1, but you have huggingface-hub 1.0.1 which is incompatible.\n",
      "googletrans 3.0.0 requires httpx==0.13.3, but you have httpx 0.28.1 which is incompatible.\n",
      "peft 0.11.1 requires torch>=1.13.0, but you have torch 1.12.1 which is incompatible.\n",
      "transformers 4.21.0 requires huggingface-hub<1.0,>=0.1.0, but you have huggingface-hub 1.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.3.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.0.1 pyarrow-21.0.0 shellingham-1.5.4 typer-slim-0.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a8c318",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /mnt/c/Users/theju/transformer-scratch/roneneldan/TinyStories/TinyStories.py or any data file in the same directory. Couldn't find 'roneneldan/TinyStories' on the Hugging Face Hub either: FileNotFoundError: Dataset 'roneneldan/TinyStories' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroneneldan/TinyStories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.9/site-packages/datasets/load.py:1759\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1755\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1756\u001b[0m )\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1759\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.9/site-packages/datasets/load.py:1496\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1495\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1496\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.9/site-packages/datasets/load.py:1214\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m-> 1214\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1215\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1216\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /mnt/c/Users/theju/transformer-scratch/roneneldan/TinyStories/TinyStories.py or any data file in the same directory. Couldn't find 'roneneldan/TinyStories' on the Hugging Face Hub either: FileNotFoundError: Dataset 'roneneldan/TinyStories' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4271c27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a742f2467a184dcb976a10c7b85d9083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8c0377de894438a0a0b2179dfc1edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f7ad89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk('data')\n",
    "ds.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192bce5",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncodingTokenizer:\n",
    "  def __init__(self,word_split = 'Ġ', padding_token = '<PAD>'):\n",
    "    self.vocab={}\n",
    "    self.inverse_vocab={}\n",
    "    self.tokens=[]\n",
    "    self.bpe_pairs={}\n",
    "    self.word_split = word_split\n",
    "    self.padding_token = padding_token\n",
    "\n",
    "  def set_vocabulary(self,word_split,special_tokens):\n",
    "    # Step 1: Setting the unique characters\n",
    "    unique_characters=[chr(i) for i in range(256)]\n",
    "\n",
    "    # Step 2: Word spli\n",
    "    # t characters\n",
    "    if word_split not in unique_characters:\n",
    "      unique_characters.append(self.word_split)\n",
    "\n",
    "    # Step 3: Special tokens\n",
    "    if special_tokens:\n",
    "      unique_characters.extend(self.special_tokens)\n",
    "\n",
    "    return unique_characters\n",
    "\n",
    "  def get_max_freq_pair(self,tokens):\n",
    "    pair_counts={}\n",
    "    pairs=[]\n",
    "    # Step 1: Gettkng all the token pairs-> (token[i],token[i+1])\n",
    "    for index in range(len(tokens)-1):\n",
    "      pairs.append((tokens[index],tokens[index+1]))\n",
    "\n",
    "    # Step 2: Getting the count of occurences of each of token pairs\n",
    "    pairs_counts=Counter(pairs)\n",
    "\n",
    "    # Step 3: Get the token pair whose count is the highest\n",
    "    max_pair=max(pairs_counts.items(),key=lambda x: x[1])[0]\n",
    "    return max_pair\n",
    "\n",
    "  def merge_tokens(self,tokens,max_pair,new_pair_id):\n",
    "    # In the tokens, check the presence of occurence of max_pair and if exists then replace max_pair with new_pair_id\n",
    "    # Eg: tokens=[87,76,44,25,38,44,25,19], max_pair=[44,25],  new_pair_id=123\n",
    "    # Output: [87,76,123,38,123,19]\n",
    "    new_tokens=[]\n",
    "    i=0\n",
    "    while i<=len(tokens)-1:\n",
    "      if i==len(tokens)-1:\n",
    "        new_tokens.append(tokens[i])\n",
    "        break\n",
    "\n",
    "      elif (tokens[i],tokens[i+1])==max_pair:\n",
    "        new_tokens.append(new_pair_id)\n",
    "        i+=2\n",
    "\n",
    "      else:\n",
    "        new_tokens.append(tokens[i])\n",
    "        i+=1\n",
    "    return new_tokens\n",
    "\n",
    "  def train(self,text,vocab_size,special_tokens):\n",
    "    if vocab_size<=258:\n",
    "      raise ValueError('Please enter a vocab size greater than 258 since this defines the basic set of characters')\n",
    "    self.special_tokens = special_tokens\n",
    "\n",
    "    # Setting the vocabulary\n",
    "    vocab = self.set_vocabulary(self.word_split,self.special_tokens)\n",
    "    for index,character in enumerate(vocab):\n",
    "      self.vocab[index]=character\n",
    "      self.inverse_vocab[character]=index\n",
    "\n",
    "    # Transforming the text\n",
    "    ## Step 1: Replacing all thw white-space character\n",
    "    processed_text=[]\n",
    "    for index,char in enumerate(text):\n",
    "      if index!=0 and char==' ':\n",
    "        processed_text.append(self.word_split)\n",
    "      if char!=' ':\n",
    "        processed_text.append(char)\n",
    "    processed_text=\"\".join(processed_text)\n",
    "\n",
    "    ## Step 2: Getting the numerical form of token\n",
    "    self.tokens=[]\n",
    "    for char in processed_text:\n",
    "      self.tokens.append(self.inverse_vocab[char])\n",
    "\n",
    "    ## Step 3: BPE-algorithm\n",
    "    vocab_length=len(self.vocab)\n",
    "    for i in range(vocab_length,vocab_size):\n",
    "      max_pair=self.get_max_freq_pair(self.tokens)\n",
    "      if max_pair is None:\n",
    "        break\n",
    "      self.bpe_pairs[max_pair]=i\n",
    "      self.tokens=self.merge_tokens(self.tokens,max_pair,i)\n",
    "\n",
    "    ## Step 4: Update vocab with BPE\n",
    "    for pair,new_index in self.bpe_pairs.items():\n",
    "      merged_token=self.vocab[pair[0]]+self.vocab[pair[1]]\n",
    "      self.vocab[new_index]=merged_token\n",
    "      self.inverse_vocab[merged_token]=new_index\n",
    "\n",
    "  def encode(self,text):\n",
    "    # Step 1: Basically tokens are split into words. Replace all the occurences of \"\\n\" to \" <NEWLINE> \". This is to avoid splitting issues.\n",
    "    tokens_split=text.replace('\\n',' <NEWLINE> ').split()\n",
    "    tokens=[]\n",
    "    for i in tokens_split:\n",
    "      if i=='<NEWLINE>':\n",
    "        tokens.append('\\n')\n",
    "      else:\n",
    "        tokens.append(i)\n",
    "\n",
    "    # Step 2: Cleaning of tokens\n",
    "    ## Eg: 'This is a ball' will be tokenized as ['The','Ġis','Ġa', 'Ġball']\n",
    "    # Ensures that all the tokens in a line other than the first one will be prefixed with \"Ġ\" to show the word boundaries\n",
    "    tokens_cleaned=[]\n",
    "    for index,token in enumerate(tokens):\n",
    "      if index>0 and not token.startswith('\\n'):\n",
    "        tokens_cleaned.append(self.word_split+token)\n",
    "      else:\n",
    "        tokens_cleaned.append(token)\n",
    "\n",
    "    # Step 3: Getting the corresponding token IDs from the cleaned tokens\n",
    "    ## Checks whether tokens exist in the vocabulary. If not, then perform BPE tokenization of the token\n",
    "    token_ids=[]\n",
    "    for token in tokens_cleaned:\n",
    "      if token in self.inverse_vocab.keys():\n",
    "        token_ids.append(self.inverse_vocab[token])\n",
    "      else:\n",
    "        token_ids.extend(self.tokenize_using_bpe(token))\n",
    "    return token_ids\n",
    "\n",
    "  def tokenize_using_bpe(self,token):\n",
    "    # Step 1: Mapping the tokens to their IDs from the vocabulary\n",
    "    token_ids=[]\n",
    "    for char in token:\n",
    "      if char in self.inverse_vocab.keys():\n",
    "        token_ids.append(self.inverse_vocab[char])\n",
    "      else:\n",
    "        token_ids.append(None)\n",
    "\n",
    "    # Step 2: Check whether token does not exist in Vocabulary- In that case stop\n",
    "    if None in token_ids:\n",
    "      token_dict=dict(zip(token_ids,token))\n",
    "      missing_characters=[]\n",
    "      for id,ch in token_dict.items():\n",
    "        if id is None:\n",
    "          missing_characters.append(ch)\n",
    "      raise ValueError(f\"No token IDs found for the characters:{missing_characters}\")\n",
    "\n",
    "    # Step 3: Now merging\n",
    "    can_merge=True\n",
    "    while can_merge and len(token_ids)>1:\n",
    "      can_merge=False\n",
    "      i=0\n",
    "      new_tokens=[]\n",
    "      \"\"\"\n",
    "      Check whether the token pair is part of bpe_pairs occured during training,\n",
    "      If yes, index = index + 2, else index = index + 1.\n",
    "      This iteration occurs until there exists no merging exists for all the tokens in token_ids.\n",
    "      No merging exists means that there are no more possible keys to merge in bpe_pairs.\n",
    "      \"\"\"\n",
    "      while i<len(token_ids)-1:\n",
    "        pair=(token_ids[i],token_ids[i+1])\n",
    "        if pair in self.bpe_pairs.keys():\n",
    "          pair_id=self.bpe_pairs[pair]\n",
    "          new_tokens.append(pair_id)\n",
    "          i+=2\n",
    "          can_merge=True\n",
    "        else:\n",
    "          new_tokens.append(token_ids[i])\n",
    "          i+=1\n",
    "      if i<len(token_ids):\n",
    "        new_tokens.append(token_ids[i])\n",
    "      token_ids=new_tokens\n",
    "\n",
    "    return token_ids\n",
    "\n",
    "  def decode(self,token_ids):\n",
    "    # Step 1: Check whether there are non-existing token IDs\n",
    "    non_existing_ids=[]\n",
    "    for id in token_ids:\n",
    "      if id not in self.vocab.keys():\n",
    "        non_existing_ids.append(id)\n",
    "    if len(non_existing_ids)>0:\n",
    "      raise ValueError(f\"No token found for the token IDs:{non_existing_ids}\")\n",
    "\n",
    "    # Step 2: Decoding- Check whether text corresponding to token ID starts with word_split-symbol('Ġ'). If yes replace word_split-symbol with \" \" else just append the text to string\n",
    "    final=\"\"\n",
    "    for id in token_ids:\n",
    "      text=self.vocab[id]\n",
    "      if text.startswith(self.word_split):\n",
    "        final+=\" \"+text[1:]\n",
    "      else:\n",
    "        final+=\"\"+text\n",
    "\n",
    "    return final\n",
    "\n",
    "  def save_bpe_vocab_and_merges(self,vocab_path,bpe_path):\n",
    "    with open(vocab_path,'w',encoding='utf-8') as f:\n",
    "      json.dump(self.vocab,f,ensure_ascii=False, indent=2)\n",
    "    with open(bpe_path,'w',encoding='utf-8') as f:\n",
    "      json.dump([{'pair':list(pair),'id':id } for pair,id in self.bpe_pairs.items()],f,\n",
    "                ensure_ascii=False, indent=2)\n",
    "\n",
    "  def load_bpe_vocab_and_merges(self,vocab_path,bpe_path):\n",
    "    with open(vocab_path,'r',encoding='utf-8') as f:\n",
    "      loaded_vocab=json.load(f)\n",
    "      self.vocab = {int(id):token for id,token in loaded_vocab.items()}\n",
    "      self.inverse_vocab={token:int(id) for id,token in self.vocab.items()}\n",
    "    with open(bpe_path,'r',encoding='utf-8') as f:\n",
    "      bpe=json.load(f)\n",
    "      for merge in bpe:\n",
    "        self.bpe_pairs[tuple(merge['pair'])]=merge['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9bb5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df = df.iloc[:,:2]\n",
    "df = df.rename(columns = {'v1':'class','v2':'text'})\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172641c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57dca240",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(n_rows*0.75)\n",
    "test_len = n_rows-train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de54ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:train_len,:]\n",
    "test_df = df.iloc[train_len:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001835e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = '\\n'.join(train_df['text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfba7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = BytePairEncodingTokenizer()\n",
    "tokenizer1.train(train_text, vocab_size=500, special_tokens={\"<|endoftext|>\",\"œ\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1bc00a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer1.encode(i)) for i in train_df['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d1c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer1.bpe_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489460ce",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_shape = (64, 512, 512)  # Example query tensor\n",
    "key_shape = (3, 512, 512)    # Example key tensor \n",
    "value_shape = (3, 512, 512)  # Example value tensor\n",
    "head_count = 8\n",
    "model_size = 512\n",
    "ffn_hidden_dimension = 4096\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a5736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, n_dim):\n",
    "        super().__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.weights = nn.Parameter(torch.zeros((vocab_size, n_dim)))\n",
    "        nn.init.uniform_(self.weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.sqrt(self.n_dim) * self.weights[x]\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Sinusoidal Positional Encoding.\n",
    "    \n",
    "    wavelength: factor to determine the wavelength in the sinusoidal function.\n",
    "    \"\"\"\n",
    "    def __init__(self, wavelength):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.wavelength = wavelength\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Given a (... x seq_len x embedding_dim) tensor, returns a (seq_len x embedding_dim) tensor.\"\"\"\n",
    "        seq_len, embedding_dim = x.shape[-2], x.shape[-1]\n",
    "        pe = torch.zeros((seq_len, embedding_dim))\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        factor = torch.exp(-math.log(self.wavelength) * torch.arange(0, embedding_dim, 2) / embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * factor)\n",
    "        pe[:, 1::2] = torch.cos(position * factor)\n",
    "        return pe\n",
    "\n",
    "class ModelEmbeddings(nn.Module):\n",
    "    def __init__(self,vocab_size, n_dim, wavelength=10000.,dropout_rate=0.1):\n",
    "        self.pe = PositionalEncoding(wavelength)\n",
    "        self.te = TransformerEmbedding(vocab_size, n_dim)\n",
    "        self.dropout = nn.DropOut(dropout_rate)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.pe(x)+self.te(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5583b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,query_shape,key_shape,value_shape,head_count,model_size,ffn_hidden_dimension,dropout_rate):\n",
    "        super().__init__()\n",
    "        self.query_shape = query_shape\n",
    "        self.key_shape = key_shape\n",
    "        self.value_shape = value_shape\n",
    "        self.head_count = head_count\n",
    "        self.model_size = model_size\n",
    "        self.ffn_hidden_dimension = ffn_hidden_dimension\n",
    "        self.multi_head = MultiHeadAttention(self.query_shape,\n",
    "                                             self.key_shape,\n",
    "                                             self.value_shape,\n",
    "                                             self.head_count,\n",
    "                                             self.model_size)\n",
    "        self.normal_layer_1 = nn.LayerNorm(self.model_size)\n",
    "        self.normal_layer_2 = nn.LayerNorm(self.model_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.ffn = FeedForwardNetwork(self.model_size,self.model_size,self.ffn_hidden_dimension)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x_input = x\n",
    "\n",
    "        multi_head_output = self.multi_head(x_input,x_input,x_input)\n",
    "        layer_norm_1_output = self.normal_layer_1(x_input + self.dropout(multi_head_output))\n",
    "\n",
    "        ffn_output = self.ffn(layer_norm_1_output)\n",
    "        encoder_output = self.normal_layer_2(layer_norm_1_output + self.dropout(ffn_output))\n",
    "        \n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9128b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "                  query_shape,\n",
    "                  key_shape,\n",
    "                  value_shape,\n",
    "                  head_count,\n",
    "                  model_size,\n",
    "                  ffn_hidden_dimension,\n",
    "                  dropout_rate\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0129a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 64, 512) \n",
    "encoder_output = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7860a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba2f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_shape = (3, 64, 512)  # Example query tensor\n",
    "key_shape = (3, 64, 512)    # Example key tensor \n",
    "value_shape = (3, 64, 512)  # Example value tensor\n",
    "head_count = 8\n",
    "model_size = 512\n",
    "ffn_hidden_dimension = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e659f697",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03979726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,query_shape,key_shape,value_shape,head_count,model_size,ffn_hidden_dimension,dropout_rate):\n",
    "        super().__init__()\n",
    "        self.query_shape = query_shape\n",
    "        self.key_shape = key_shape\n",
    "        self.value_shape = value_shape\n",
    "        self.head_count = head_count\n",
    "        self.model_size = model_size\n",
    "        self.ffn_hidden_dimension = ffn_hidden_dimension\n",
    "        self.multi_head = MultiHeadAttention(self.query_shape,\n",
    "                                             self.key_shape,\n",
    "                                             self.value_shape,\n",
    "                                             self.head_count,\n",
    "                                             self.model_size)\n",
    "        self.masked_multi_head = MultiHeadAttention(self.query_shape,\n",
    "                                             self.key_shape,\n",
    "                                             self.value_shape,\n",
    "                                             self.head_count,\n",
    "                                             self.model_size)\n",
    "        self.normal_layer_1 = nn.LayerNorm(self.model_size)\n",
    "        self.normal_layer_2 = nn.LayerNorm(self.model_size)\n",
    "        self.normal_layer_3 = nn.LayerNorm(self.model_size)\n",
    "        self.ffn = FeedForwardNetwork(self.model_size,self.model_size,\n",
    "                                         self.ffn_hidden_dimension)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self,x, encoder_output):\n",
    "\n",
    "        \"\"\"\n",
    "        1. Masked Multi-Head Attention: \n",
    "        This is mainly used to prevent overlooking for futuristic values or predictions of the output. \n",
    "        \"\"\"\n",
    "        x_input = x\n",
    "        decoder_ip_sentence_length = x_input.shape[-2]\n",
    "        mask = torch.triu(torch.ones((decoder_ip_sentence_length,\n",
    "                                      decoder_ip_sentence_length)),diagonal=1)\n",
    "\n",
    "        masked_attention = self.masked_multi_head(x_input,x_input,x_input,attention_mask = mask)\n",
    "        layer_norm_1_output = self.normal_layer_1(x_input + self.dropout(masked_attention))\n",
    "\n",
    "        \"\"\"\n",
    "        2. Multi-Head Attention: \n",
    "        This is mainly used to prevent overlooking for futuristic values or predictions of the output. \n",
    "        Query-> Decoder based \n",
    "        Key-> From the output of encoder\n",
    "        Value-> From the output of encoder\n",
    "        \"\"\"\n",
    "        multi_head_attention = self.multi_head(layer_norm_1_output,encoder_output,encoder_output)\n",
    "        layer_norm_2_output = self.normal_layer_2(layer_norm_1_output + self.dropout(multi_head_attention))\n",
    "\n",
    "        # 3. Feed-forward Network\n",
    "        decoder_output = self.normal_layer_3(layer_norm_2_output + self.ffn(layer_norm_2_output))\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08ccfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "                  query_shape,\n",
    "                  key_shape,\n",
    "                  value_shape,\n",
    "                  head_count,\n",
    "                  model_size,\n",
    "                  ffn_hidden_dimension,\n",
    "                  dropout_rate\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "149c68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    def __init__(self, num_layers, source_vocab_size, target_vocab_size, query_shape, key_shape, value_shape, head_count, model_size, ffn_hidden_dimension, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = [Encoder(query_shape, \n",
    "                                key_shape, \n",
    "                                value_shape, \n",
    "                                head_count, \n",
    "                                model_size, \n",
    "                                ffn_hidden_dimension, \n",
    "                                dropout_rate) for _ in range(num_layers)]\n",
    "        self.decoder_layers = [Decoder(query_shape, \n",
    "                               key_shape, \n",
    "                               value_shape, \n",
    "                               head_count, \n",
    "                               model_size, \n",
    "                               ffn_hidden_dimension, \n",
    "                               dropout_rate) for _ in range(num_layers)]\n",
    "        self.embeddings_ip = ModelEmbeddings(source_vocab_size,model_size)\n",
    "        self.embeddings_op = ModelEmbeddings(target_vocab_size,model_size)\n",
    "\n",
    "        self.ffn = nn.Linear(model_size,target_vocab_size)\n",
    "    \n",
    "    def forward(self,input_text,output_text):\n",
    "        \n",
    "        ip_embedding = self.embeddings_ip(input_text)\n",
    "        op_embedding = self.embeddings_ip(output_text)\n",
    "\n",
    "        encoder_output = ip_embedding\n",
    "        for encoder in self.encoder_layers:\n",
    "            encoder_output = encoder(ip_embedding)\n",
    "        \n",
    "        decoder_output = op_embedding\n",
    "        for decoder in self.decoder_layers:\n",
    "            decoder_output = decoder(op_embedding, encoder_output)\n",
    "\n",
    "        output = self.ffn(decoder_output)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
