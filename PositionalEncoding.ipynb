{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "The positional encoding module computes a sequence of embeddings based on the position of an embedding from the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Sinusoidal Positional Encoding.\n",
    "    \n",
    "    wavelength: factor to determine the wavelength in the sinusoidal function.\n",
    "    \"\"\"\n",
    "    def __init__(self, wavelength=10000.):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.wavelength = wavelength\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Given a (... x seq_len x embedding_dim) tensor, returns a (seq_len x embedding_dim) tensor.\"\"\"\n",
    "        seq_len, embedding_dim = x.shape[-2], x.shape[-1]\n",
    "        pe = torch.zeros((seq_len, embedding_dim))\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        factor = torch.exp(-math.log(self.wavelength) * torch.arange(0, embedding_dim, 2) / embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * factor)\n",
    "        pe[:, 1::2] = torch.cos(position * factor)\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403],\n",
       "        [ 0.9093, -0.4161],\n",
       "        [ 0.1411, -0.9900],\n",
       "        [-0.7568, -0.6536],\n",
       "        [-0.9589,  0.2837],\n",
       "        [-0.2794,  0.9602],\n",
       "        [ 0.6570,  0.7539]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare parameters\n",
    "batch_size = 4\n",
    "seq_length = 8\n",
    "embedding_dim = 2\n",
    "\n",
    "pe = PositionalEncoding()\n",
    "x = torch.rand((batch_size, seq_length, embedding_dim))\n",
    "pe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8380,  1.1593],\n",
       "         [ 0.8669,  0.6860],\n",
       "         [ 1.8222,  0.1615],\n",
       "         [ 0.5154, -0.7971],\n",
       "         [-0.4513, -0.1661],\n",
       "         [-0.0788,  0.3784],\n",
       "         [ 0.0910,  1.9162],\n",
       "         [ 1.6072,  1.2504]],\n",
       "\n",
       "        [[ 0.8504,  1.0896],\n",
       "         [ 0.8801,  1.1977],\n",
       "         [ 1.2070, -0.2350],\n",
       "         [ 0.6507, -0.7666],\n",
       "         [-0.4654, -0.1322],\n",
       "         [-0.6249,  1.1393],\n",
       "         [-0.2041,  1.9036],\n",
       "         [ 1.3226,  1.5797]],\n",
       "\n",
       "        [[ 0.5802,  1.4557],\n",
       "         [ 1.3878,  1.2987],\n",
       "         [ 1.4476, -0.4103],\n",
       "         [ 1.1407, -0.9080],\n",
       "         [-0.6525, -0.6126],\n",
       "         [-0.8150,  1.0617],\n",
       "         [ 0.1906,  1.5095],\n",
       "         [ 1.1177,  0.9003]],\n",
       "\n",
       "        [[ 0.6944,  1.0443],\n",
       "         [ 1.7252,  1.5122],\n",
       "         [ 1.0109,  0.3975],\n",
       "         [ 0.5423, -0.6782],\n",
       "         [-0.7034, -0.1417],\n",
       "         [-0.8156,  0.8136],\n",
       "         [-0.2586,  1.9527],\n",
       "         [ 1.6237,  0.8781]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# position-encoded tensor\n",
    "x += pe(x)\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
