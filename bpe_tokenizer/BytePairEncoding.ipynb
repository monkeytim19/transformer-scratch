{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bUCse4WG-gM"
      },
      "source": [
        "[BPE Blog](https://sebastianraschka.com/blog/2025/bpe-from-scratch.html)\n",
        "\n",
        "[Research Paper](https://aclanthology.org/P16-1162.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CpRLnLkZf8ig"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JKvHz5tB_3Fz"
      },
      "outputs": [],
      "source": [
        "class BytePairEncodingTokenizer:\n",
        "  def __init__(self,word_split = 'Ġ'):\n",
        "    self.vocab={}\n",
        "    self.inverse_vocab={}\n",
        "    self.tokens=[]\n",
        "    self.bpe_pairs={}\n",
        "    self.word_split = word_split\n",
        "\n",
        "  def set_vocabulary(self,word_split,special_tokens):\n",
        "    # Step 1: Setting the unique characters\n",
        "    unique_characters=[chr(i) for i in range(256)]\n",
        "\n",
        "    # Step 2: Word spli\n",
        "    # t characters\n",
        "    if word_split not in unique_characters:\n",
        "      unique_characters.append(self.word_split)\n",
        "\n",
        "    # Step 3: Special tokens\n",
        "    if special_tokens:\n",
        "      unique_characters.extend(self.special_tokens)\n",
        "\n",
        "    return unique_characters\n",
        "\n",
        "  def get_max_freq_pair(self,tokens):\n",
        "    pair_counts={}\n",
        "    pairs=[]\n",
        "    # Step 1: Gettkng all the token pairs-> (token[i],token[i+1])\n",
        "    for index in range(len(tokens)-1):\n",
        "      pairs.append((tokens[index],tokens[index+1]))\n",
        "\n",
        "    # Step 2: Getting the count of occurences of each of token pairs\n",
        "    pairs_counts=Counter(pairs)\n",
        "\n",
        "    # Step 3: Get the token pair whose count is the highest\n",
        "    max_pair=max(pairs_counts.items(),key=lambda x: x[1])[0]\n",
        "    return max_pair\n",
        "\n",
        "  def merge_tokens(self,tokens,max_pair,new_pair_id):\n",
        "    # In the tokens, check the presence of occurence of max_pair and if exists then replace max_pair with new_pair_id\n",
        "    # Eg: tokens=[87,76,44,25,38,44,25,19], max_pair=[44,25],  new_pair_id=123\n",
        "    # Output: [87,76,123,38,123,19]\n",
        "    new_tokens=[]\n",
        "    i=0\n",
        "    while i<=len(tokens)-1:\n",
        "      if i==len(tokens)-1:\n",
        "        new_tokens.append(tokens[i])\n",
        "        break\n",
        "\n",
        "      elif (tokens[i],tokens[i+1])==max_pair:\n",
        "        new_tokens.append(new_pair_id)\n",
        "        i+=2\n",
        "\n",
        "      else:\n",
        "        new_tokens.append(tokens[i])\n",
        "        i+=1\n",
        "    return new_tokens\n",
        "\n",
        "  def train(self,text,vocab_size,special_tokens):\n",
        "    if vocab_size<=258:\n",
        "      raise ValueError('Please enter a vocab size greater than 258 since this defines the basic set of characters')\n",
        "    self.special_tokens = special_tokens\n",
        "\n",
        "    # Setting the vocabulary\n",
        "    vocab = self.set_vocabulary(self.word_split,self.special_tokens)\n",
        "    for index,character in enumerate(vocab):\n",
        "      self.vocab[index]=character\n",
        "      self.inverse_vocab[character]=index\n",
        "\n",
        "    # Transforming the text\n",
        "    ## Step 1: Replacing all thw white-space character\n",
        "    processed_text=[]\n",
        "    for index,char in enumerate(text):\n",
        "      if index!=0 and char==' ':\n",
        "        processed_text.append(self.word_split)\n",
        "      if char!=' ':\n",
        "        processed_text.append(char)\n",
        "    processed_text=\"\".join(processed_text)\n",
        "\n",
        "    ## Step 2: Getting the numerical form of token\n",
        "    self.tokens=[]\n",
        "    for char in processed_text:\n",
        "      self.tokens.append(self.inverse_vocab[char])\n",
        "\n",
        "    ## Step 3: BPE-algorithm\n",
        "    vocab_length=len(self.vocab)\n",
        "    for i in range(vocab_length,vocab_size):\n",
        "      max_pair=self.get_max_freq_pair(self.tokens)\n",
        "      if max_pair is None:\n",
        "        break\n",
        "      self.bpe_pairs[max_pair]=i\n",
        "      self.tokens=self.merge_tokens(self.tokens,max_pair,i)\n",
        "\n",
        "    ## Step 4: Update vocab with BPE\n",
        "    for pair,new_index in self.bpe_pairs.items():\n",
        "      merged_token=self.vocab[pair[0]]+self.vocab[pair[1]]\n",
        "      self.vocab[new_index]=merged_token\n",
        "      self.inverse_vocab[merged_token]=new_index\n",
        "\n",
        "  def encode(self,text):\n",
        "    # Step 1: Basically tokens are split into words. Replace all the occurences of \"\\n\" to \" <NEWLINE> \". This is to avoid splitting issues.\n",
        "    tokens_split=text.replace('\\n',' <NEWLINE> ').split()\n",
        "    tokens=[]\n",
        "    for i in tokens_split:\n",
        "      if i=='<NEWLINE>':\n",
        "        tokens.append('\\n')\n",
        "      else:\n",
        "        tokens.append(i)\n",
        "\n",
        "    # Step 2: Cleaning of tokens\n",
        "    ## Eg: 'This is a ball' will be tokenized as ['The','Ġis','Ġa', 'Ġball']\n",
        "    # Ensures that all the tokens in a line other than the first one will be prefixed with \"Ġ\" to show the word boundaries\n",
        "    tokens_cleaned=[]\n",
        "    for index,token in enumerate(tokens):\n",
        "      if index>0 and not token.startswith('\\n'):\n",
        "        tokens_cleaned.append(self.word_split+token)\n",
        "      else:\n",
        "        tokens_cleaned.append(token)\n",
        "\n",
        "    # Step 3: Getting the corresponding token IDs from the cleaned tokens\n",
        "    ## Checks whether tokens exist in the vocabulary. If not, then perform BPE tokenization of the token\n",
        "    token_ids=[]\n",
        "    for token in tokens_cleaned:\n",
        "      if token in self.inverse_vocab.keys():\n",
        "        token_ids.append(self.inverse_vocab[token])\n",
        "      else:\n",
        "        token_ids.extend(self.tokenize_using_bpe(token))\n",
        "    return token_ids\n",
        "\n",
        "  def tokenize_using_bpe(self,token):\n",
        "    # Step 1: Mapping the tokens to their IDs from the vocabulary\n",
        "    token_ids=[]\n",
        "    for char in token:\n",
        "      if char in self.inverse_vocab.keys():\n",
        "        token_ids.append(self.inverse_vocab[char])\n",
        "      else:\n",
        "        token_ids.append(None)\n",
        "\n",
        "    # Step 2: Check whether token does not exist in Vocabulary- In that case stop\n",
        "    if None in token_ids:\n",
        "      token_dict=dict(zip(token_ids,token))\n",
        "      missing_characters=[]\n",
        "      for id,ch in token_dict.items():\n",
        "        if id is None:\n",
        "          missing_characters.append(ch)\n",
        "      raise ValueError(f\"No token IDs found for the characters:{missing_characters}\")\n",
        "\n",
        "    # Step 3: Now merging\n",
        "    can_merge=True\n",
        "    while can_merge and len(token_ids)>1:\n",
        "      can_merge=False\n",
        "      i=0\n",
        "      new_tokens=[]\n",
        "      \"\"\"\n",
        "      Check whether the token pair is part of bpe_pairs occured during training,\n",
        "      If yes, index = index + 2, else index = index + 1.\n",
        "      This iteration occurs until there exists no merging exists for all the tokens in token_ids.\n",
        "      No merging exists means that there are no more possible keys to merge in bpe_pairs.\n",
        "      \"\"\"\n",
        "      while i<len(token_ids)-1:\n",
        "        pair=(token_ids[i],token_ids[i+1])\n",
        "        if pair in self.bpe_pairs.keys():\n",
        "          pair_id=self.bpe_pairs[pair]\n",
        "          new_tokens.append(pair_id)\n",
        "          i+=2\n",
        "          can_merge=True\n",
        "        else:\n",
        "          new_tokens.append(token_ids[i])\n",
        "          i+=1\n",
        "      if i<len(token_ids):\n",
        "        new_tokens.append(token_ids[i])\n",
        "      token_ids=new_tokens\n",
        "\n",
        "    return token_ids\n",
        "\n",
        "  def decode(self,token_ids):\n",
        "    # Step 1: Check whether there are non-existing token IDs\n",
        "    non_existing_ids=[]\n",
        "    for id in token_ids:\n",
        "      if id not in self.vocab.keys():\n",
        "        non_existing_ids.append(id)\n",
        "    if len(non_existing_ids)>0:\n",
        "      raise ValueError(f\"No token found for the token IDs:{non_existing_ids}\")\n",
        "\n",
        "    # Step 2: Decoding- Check whether text corresponding to token ID starts with word_split-symbol('Ġ'). If yes replace word_split-symbol with \" \" else just append the text to string\n",
        "    final=\"\"\n",
        "    for id in token_ids:\n",
        "      text=self.vocab[id]\n",
        "      if text.startswith(self.word_split):\n",
        "        final+=\" \"+text[1:]\n",
        "      else:\n",
        "        final+=\"\"+text\n",
        "\n",
        "    return final\n",
        "\n",
        "  def save_bpe_vocab_and_merges(self,vocab_path,bpe_path):\n",
        "    with open(vocab_path,'w',encoding='utf-8') as f:\n",
        "      json.dump(self.vocab,f,ensure_ascii=False, indent=2)\n",
        "    with open(bpe_path,'w',encoding='utf-8') as f:\n",
        "      json.dump([{'pair':list(pair),'id':id } for pair,id in self.bpe_pairs.items()],f,\n",
        "                ensure_ascii=False, indent=2)\n",
        "\n",
        "  def load_bpe_vocab_and_merges(self,vocab_path,bpe_path):\n",
        "    with open(vocab_path,'r',encoding='utf-8') as f:\n",
        "      loaded_vocab=json.load(f)\n",
        "      self.vocab = {int(id):token for id,token in loaded_vocab.items()}\n",
        "      self.inverse_vocab={token:int(id) for id,token in self.vocab.items()}\n",
        "    with open(bpe_path,'r',encoding='utf-8') as f:\n",
        "      bpe=json.load(f)\n",
        "      for merge in bpe:\n",
        "        self.bpe_pairs[tuple(merge['pair'])]=merge['id']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMdYBakygQKG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  class                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni..."
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "df = df.iloc[:,:2]\n",
        "df = df.rename(columns = {'v1':'class','v2':'text'})\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_rows, n_cols = df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_len = int(n_rows*0.75)\n",
        "test_len = n_rows-train_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = df.iloc[:train_len,:]\n",
        "test = df.iloc[train_len:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_text = '\\n'.join(train['text'].to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Kdno3cSgSve"
      },
      "outputs": [],
      "source": [
        "# Training Byte Pair Encoding\n",
        "tokenizer1 = BytePairEncodingTokenizer()\n",
        "tokenizer1.train(train_text, vocab_size=500, special_tokens={\"<|endoftext|>\",\"œ\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq6H_qJ59Tul",
        "outputId": "70be5d04-a89c-496f-ef97-7caa51d48b44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '\\x00',\n",
              " 1: '\\x01',\n",
              " 2: '\\x02',\n",
              " 3: '\\x03',\n",
              " 4: '\\x04',\n",
              " 5: '\\x05',\n",
              " 6: '\\x06',\n",
              " 7: '\\x07',\n",
              " 8: '\\x08',\n",
              " 9: '\\t',\n",
              " 10: '\\n',\n",
              " 11: '\\x0b',\n",
              " 12: '\\x0c',\n",
              " 13: '\\r',\n",
              " 14: '\\x0e',\n",
              " 15: '\\x0f',\n",
              " 16: '\\x10',\n",
              " 17: '\\x11',\n",
              " 18: '\\x12',\n",
              " 19: '\\x13',\n",
              " 20: '\\x14',\n",
              " 21: '\\x15',\n",
              " 22: '\\x16',\n",
              " 23: '\\x17',\n",
              " 24: '\\x18',\n",
              " 25: '\\x19',\n",
              " 26: '\\x1a',\n",
              " 27: '\\x1b',\n",
              " 28: '\\x1c',\n",
              " 29: '\\x1d',\n",
              " 30: '\\x1e',\n",
              " 31: '\\x1f',\n",
              " 32: ' ',\n",
              " 33: '!',\n",
              " 34: '\"',\n",
              " 35: '#',\n",
              " 36: '$',\n",
              " 37: '%',\n",
              " 38: '&',\n",
              " 39: \"'\",\n",
              " 40: '(',\n",
              " 41: ')',\n",
              " 42: '*',\n",
              " 43: '+',\n",
              " 44: ',',\n",
              " 45: '-',\n",
              " 46: '.',\n",
              " 47: '/',\n",
              " 48: '0',\n",
              " 49: '1',\n",
              " 50: '2',\n",
              " 51: '3',\n",
              " 52: '4',\n",
              " 53: '5',\n",
              " 54: '6',\n",
              " 55: '7',\n",
              " 56: '8',\n",
              " 57: '9',\n",
              " 58: ':',\n",
              " 59: ';',\n",
              " 60: '<',\n",
              " 61: '=',\n",
              " 62: '>',\n",
              " 63: '?',\n",
              " 64: '@',\n",
              " 65: 'A',\n",
              " 66: 'B',\n",
              " 67: 'C',\n",
              " 68: 'D',\n",
              " 69: 'E',\n",
              " 70: 'F',\n",
              " 71: 'G',\n",
              " 72: 'H',\n",
              " 73: 'I',\n",
              " 74: 'J',\n",
              " 75: 'K',\n",
              " 76: 'L',\n",
              " 77: 'M',\n",
              " 78: 'N',\n",
              " 79: 'O',\n",
              " 80: 'P',\n",
              " 81: 'Q',\n",
              " 82: 'R',\n",
              " 83: 'S',\n",
              " 84: 'T',\n",
              " 85: 'U',\n",
              " 86: 'V',\n",
              " 87: 'W',\n",
              " 88: 'X',\n",
              " 89: 'Y',\n",
              " 90: 'Z',\n",
              " 91: '[',\n",
              " 92: '\\\\',\n",
              " 93: ']',\n",
              " 94: '^',\n",
              " 95: '_',\n",
              " 96: '`',\n",
              " 97: 'a',\n",
              " 98: 'b',\n",
              " 99: 'c',\n",
              " 100: 'd',\n",
              " 101: 'e',\n",
              " 102: 'f',\n",
              " 103: 'g',\n",
              " 104: 'h',\n",
              " 105: 'i',\n",
              " 106: 'j',\n",
              " 107: 'k',\n",
              " 108: 'l',\n",
              " 109: 'm',\n",
              " 110: 'n',\n",
              " 111: 'o',\n",
              " 112: 'p',\n",
              " 113: 'q',\n",
              " 114: 'r',\n",
              " 115: 's',\n",
              " 116: 't',\n",
              " 117: 'u',\n",
              " 118: 'v',\n",
              " 119: 'w',\n",
              " 120: 'x',\n",
              " 121: 'y',\n",
              " 122: 'z',\n",
              " 123: '{',\n",
              " 124: '|',\n",
              " 125: '}',\n",
              " 126: '~',\n",
              " 127: '\\x7f',\n",
              " 128: '\\x80',\n",
              " 129: '\\x81',\n",
              " 130: '\\x82',\n",
              " 131: '\\x83',\n",
              " 132: '\\x84',\n",
              " 133: '\\x85',\n",
              " 134: '\\x86',\n",
              " 135: '\\x87',\n",
              " 136: '\\x88',\n",
              " 137: '\\x89',\n",
              " 138: '\\x8a',\n",
              " 139: '\\x8b',\n",
              " 140: '\\x8c',\n",
              " 141: '\\x8d',\n",
              " 142: '\\x8e',\n",
              " 143: '\\x8f',\n",
              " 144: '\\x90',\n",
              " 145: '\\x91',\n",
              " 146: '\\x92',\n",
              " 147: '\\x93',\n",
              " 148: '\\x94',\n",
              " 149: '\\x95',\n",
              " 150: '\\x96',\n",
              " 151: '\\x97',\n",
              " 152: '\\x98',\n",
              " 153: '\\x99',\n",
              " 154: '\\x9a',\n",
              " 155: '\\x9b',\n",
              " 156: '\\x9c',\n",
              " 157: '\\x9d',\n",
              " 158: '\\x9e',\n",
              " 159: '\\x9f',\n",
              " 160: '\\xa0',\n",
              " 161: '¡',\n",
              " 162: '¢',\n",
              " 163: '£',\n",
              " 164: '¤',\n",
              " 165: '¥',\n",
              " 166: '¦',\n",
              " 167: '§',\n",
              " 168: '¨',\n",
              " 169: '©',\n",
              " 170: 'ª',\n",
              " 171: '«',\n",
              " 172: '¬',\n",
              " 173: '\\xad',\n",
              " 174: '®',\n",
              " 175: '¯',\n",
              " 176: '°',\n",
              " 177: '±',\n",
              " 178: '²',\n",
              " 179: '³',\n",
              " 180: '´',\n",
              " 181: 'µ',\n",
              " 182: '¶',\n",
              " 183: '·',\n",
              " 184: '¸',\n",
              " 185: '¹',\n",
              " 186: 'º',\n",
              " 187: '»',\n",
              " 188: '¼',\n",
              " 189: '½',\n",
              " 190: '¾',\n",
              " 191: '¿',\n",
              " 192: 'À',\n",
              " 193: 'Á',\n",
              " 194: 'Â',\n",
              " 195: 'Ã',\n",
              " 196: 'Ä',\n",
              " 197: 'Å',\n",
              " 198: 'Æ',\n",
              " 199: 'Ç',\n",
              " 200: 'È',\n",
              " 201: 'É',\n",
              " 202: 'Ê',\n",
              " 203: 'Ë',\n",
              " 204: 'Ì',\n",
              " 205: 'Í',\n",
              " 206: 'Î',\n",
              " 207: 'Ï',\n",
              " 208: 'Ð',\n",
              " 209: 'Ñ',\n",
              " 210: 'Ò',\n",
              " 211: 'Ó',\n",
              " 212: 'Ô',\n",
              " 213: 'Õ',\n",
              " 214: 'Ö',\n",
              " 215: '×',\n",
              " 216: 'Ø',\n",
              " 217: 'Ù',\n",
              " 218: 'Ú',\n",
              " 219: 'Û',\n",
              " 220: 'Ü',\n",
              " 221: 'Ý',\n",
              " 222: 'Þ',\n",
              " 223: 'ß',\n",
              " 224: 'à',\n",
              " 225: 'á',\n",
              " 226: 'â',\n",
              " 227: 'ã',\n",
              " 228: 'ä',\n",
              " 229: 'å',\n",
              " 230: 'æ',\n",
              " 231: 'ç',\n",
              " 232: 'è',\n",
              " 233: 'é',\n",
              " 234: 'ê',\n",
              " 235: 'ë',\n",
              " 236: 'ì',\n",
              " 237: 'í',\n",
              " 238: 'î',\n",
              " 239: 'ï',\n",
              " 240: 'ð',\n",
              " 241: 'ñ',\n",
              " 242: 'ò',\n",
              " 243: 'ó',\n",
              " 244: 'ô',\n",
              " 245: 'õ',\n",
              " 246: 'ö',\n",
              " 247: '÷',\n",
              " 248: 'ø',\n",
              " 249: 'ù',\n",
              " 250: 'ú',\n",
              " 251: 'û',\n",
              " 252: 'ü',\n",
              " 253: 'ý',\n",
              " 254: 'þ',\n",
              " 255: 'ÿ',\n",
              " 256: 'Ġ',\n",
              " 257: '<|endoftext|>',\n",
              " 258: 'œ',\n",
              " 259: 'eĠ',\n",
              " 260: 'tĠ',\n",
              " 261: 'sĠ',\n",
              " 262: 'in',\n",
              " 263: 'Ġt',\n",
              " 264: 'ou',\n",
              " 265: '.Ġ',\n",
              " 266: 'yĠ',\n",
              " 267: 'rĠ',\n",
              " 268: 'dĠ',\n",
              " 269: 'oĠ',\n",
              " 270: 'an',\n",
              " 271: 'on',\n",
              " 272: 'll',\n",
              " 273: 'en',\n",
              " 274: 'you',\n",
              " 275: 'ing',\n",
              " 276: 'er',\n",
              " 277: 'th',\n",
              " 278: '..',\n",
              " 279: 'ha',\n",
              " 280: 're',\n",
              " 281: 'Ġa',\n",
              " 282: 'Ġw',\n",
              " 283: 'or',\n",
              " 284: 'Ġth',\n",
              " 285: 'Ġs',\n",
              " 286: 'om',\n",
              " 287: '.\\n',\n",
              " 288: 'llĠ',\n",
              " 289: 'ow',\n",
              " 290: 'ar',\n",
              " 291: 'Ġm',\n",
              " 292: 'Ġc',\n",
              " 293: 'ĠtoĠ',\n",
              " 294: ',Ġ',\n",
              " 295: 'veĠ',\n",
              " 296: 'isĠ',\n",
              " 297: 'at',\n",
              " 298: 'ingĠ',\n",
              " 299: 'erĠ',\n",
              " 300: 'es',\n",
              " 301: 'ee',\n",
              " 302: 'aĠ',\n",
              " 303: 'orĠ',\n",
              " 304: 'it',\n",
              " 305: 'ea',\n",
              " 306: '!Ġ',\n",
              " 307: 'ch',\n",
              " 308: 'st',\n",
              " 309: 'Ġd',\n",
              " 310: 'andĠ',\n",
              " 311: 'al',\n",
              " 312: 'ay',\n",
              " 313: 'mĠ',\n",
              " 314: 'ĠtheĠ',\n",
              " 315: 'toĠ',\n",
              " 316: 'kĠ',\n",
              " 317: 'pl',\n",
              " 318: 'of',\n",
              " 319: 'youĠ',\n",
              " 320: '00',\n",
              " 321: 'go',\n",
              " 322: 'ri',\n",
              " 323: 'stĠ',\n",
              " 324: '.ĠI',\n",
              " 325: 'edĠ',\n",
              " 326: 'li',\n",
              " 327: 'now',\n",
              " 328: '?Ġ',\n",
              " 329: '?\\n',\n",
              " 330: 'gh',\n",
              " 331: 'meĠ',\n",
              " 332: 'ge',\n",
              " 333: 'lo',\n",
              " 334: 'ti',\n",
              " 335: 't;',\n",
              " 336: 'inĠ',\n",
              " 337: 'allĠ',\n",
              " 338: 'is',\n",
              " 339: 'ic',\n",
              " 340: 'atĠ',\n",
              " 341: 'ac',\n",
              " 342: 'un',\n",
              " 343: 'itĠ',\n",
              " 344: 'esĠ',\n",
              " 345: 'il',\n",
              " 346: 'Ġf',\n",
              " 347: 'ayĠ',\n",
              " 348: 'no',\n",
              " 349: 'hatĠ',\n",
              " 350: 'reĠ',\n",
              " 351: '...Ġ',\n",
              " 352: 'enĠ',\n",
              " 353: 'op',\n",
              " 354: 'keĠ',\n",
              " 355: 'ec',\n",
              " 356: 'all',\n",
              " 357: 'yourĠ',\n",
              " 358: 'haveĠ',\n",
              " 359: 'bu',\n",
              " 360: 'oo',\n",
              " 361: 'ex',\n",
              " 362: \"'sĠ\",\n",
              " 363: 'Th',\n",
              " 364: 'be',\n",
              " 365: 'omeĠ',\n",
              " 366: 'EĠ',\n",
              " 367: 'wi',\n",
              " 368: 'onĠ',\n",
              " 369: 'anĠ',\n",
              " 370: 'el',\n",
              " 371: 'as',\n",
              " 372: 'beĠ',\n",
              " 373: 'fĠ',\n",
              " 374: '50',\n",
              " 375: 'up',\n",
              " 376: 'forĠ',\n",
              " 377: 'oneĠ',\n",
              " 378: 'am',\n",
              " 379: 'IĠ',\n",
              " 380: 'theĠ',\n",
              " 381: 'iĠ',\n",
              " 382: 'e.Ġ',\n",
              " 383: 'outĠ',\n",
              " 384: 'You',\n",
              " 385: 'ed',\n",
              " 386: 'ustĠ',\n",
              " 387: 'ereĠ',\n",
              " 388: 'ob',\n",
              " 389: 'ev',\n",
              " 390: 'wh',\n",
              " 391: 'seĠ',\n",
              " 392: 'ad',\n",
              " 393: \"I'\",\n",
              " 394: 'od',\n",
              " 395: 'uĠ',\n",
              " 396: 'id',\n",
              " 397: 'Ġwi',\n",
              " 398: '...\\n',\n",
              " 399: 'ent',\n",
              " 400: 'ol',\n",
              " 401: \"'tĠ\",\n",
              " 402: 'getĠ',\n",
              " 403: 'notĠ',\n",
              " 404: 'asĠ',\n",
              " 405: 'et',\n",
              " 406: 'lyĠ',\n",
              " 407: 'em',\n",
              " 408: 'Ġ&',\n",
              " 409: 'igh',\n",
              " 410: '08',\n",
              " 411: 'å£',\n",
              " 412: 'lt;',\n",
              " 413: 'gt;',\n",
              " 414: '&gt;',\n",
              " 415: 'myĠ',\n",
              " 416: 'ur',\n",
              " 417: 'Ġp',\n",
              " 418: 'sh',\n",
              " 419: 'pp',\n",
              " 420: 'ear',\n",
              " 421: 'rom',\n",
              " 422: 'tsĠ',\n",
              " 423: 'ĠmyĠ',\n",
              " 424: 'orr',\n",
              " 425: 'ab',\n",
              " 426: '2Ġ',\n",
              " 427: 'la',\n",
              " 428: '\\nH',\n",
              " 429: 'butĠ',\n",
              " 430: 'urĠ',\n",
              " 431: 'ir',\n",
              " 432: 'lt;#',\n",
              " 433: 'lt;#&gt;',\n",
              " 434: 'im',\n",
              " 435: 'thĠ',\n",
              " 436: 'entĠ',\n",
              " 437: 'know',\n",
              " 438: 'eyĠ',\n",
              " 439: '\\nI',\n",
              " 440: 'ok',\n",
              " 441: 'antĠ',\n",
              " 442: 'ess',\n",
              " 443: 'and',\n",
              " 444: 'ldĠ',\n",
              " 445: 'ofĠ',\n",
              " 446: 'areĠ',\n",
              " 447: 'end',\n",
              " 448: 'ome',\n",
              " 449: 'ĠaĠ',\n",
              " 450: 'akeĠ',\n",
              " 451: 'lt;#&gt;Ġ',\n",
              " 452: ':)',\n",
              " 453: 'lat',\n",
              " 454: 'ew',\n",
              " 455: 'justĠ',\n",
              " 456: 'ro',\n",
              " 457: '09',\n",
              " 458: 'oun',\n",
              " 459: 'idĠ',\n",
              " 460: 'Ġwh',\n",
              " 461: 'TĠ',\n",
              " 462: 'endĠ',\n",
              " 463: 'ĠthatĠ',\n",
              " 464: 'hĠ',\n",
              " 465: 'callĠ',\n",
              " 466: 'ouldĠ',\n",
              " 467: 'for',\n",
              " 468: 'co',\n",
              " 469: '\\nS',\n",
              " 470: 'le',\n",
              " 471: 'UĠ',\n",
              " 472: '\\nW',\n",
              " 473: 'wor',\n",
              " 474: 'likeĠ',\n",
              " 475: 'gotĠ',\n",
              " 476: 'extĠ',\n",
              " 477: 'ra',\n",
              " 478: 'how',\n",
              " 479: 'tt',\n",
              " 480: 'Ġ&lt;#&gt;Ġ',\n",
              " 481: 'ĠareĠ',\n",
              " 482: '..Ġ',\n",
              " 483: 'heĠ',\n",
              " 484: 'ell',\n",
              " 485: ':Ġ',\n",
              " 486: 'ver',\n",
              " 487: 'RE',\n",
              " 488: 'chĠ',\n",
              " 489: 'com',\n",
              " 490: 'ag',\n",
              " 491: '4Ġ',\n",
              " 492: 'from',\n",
              " 493: 'soĠ',\n",
              " 494: 's.Ġ',\n",
              " 495: 'day',\n",
              " 496: 'leĠ',\n",
              " 497: 'um',\n",
              " 498: 'to',\n",
              " 499: 'ĠtoĠ8'}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vocabulary\n",
        "tokenizer1.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCwxtqB1gtkr",
        "outputId": "1d73ccc9-3b07-4674-9272-844f8433f423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "print(len(tokenizer1.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQnpoYJSgvk6",
        "outputId": "b7939144-4591-4dd1-d36c-636c09c5844f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "241\n"
          ]
        }
      ],
      "source": [
        "print(len(tokenizer1.bpe_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsf1QSMfg1Qr",
        "outputId": "1a4bc556-a305-4d0a-c909-f2c4b29260d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[74, 341, 107, 256, 407, 98, 477, 99, 385, 256, 364, 97, 117, 116, 121, 284, 456, 117, 330, 281, 114, 116, 281, 110, 100, 256, 326, 102, 101, 46]\n"
          ]
        }
      ],
      "source": [
        "# Encoding\n",
        "input_text = \"Jack embraced beauty through art and life.\"\n",
        "token_ids = tokenizer1.encode(input_text)\n",
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI5FK_DCqbjP",
        "outputId": "af2ec113-54f5-49fe-a76c-4137ad866667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jack embraced beauty through art and life.\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer1.decode(token_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxLp4CTvqkR4",
        "outputId": "514db531-acdf-44f9-da91-7d3b5e79a0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "74 -> J\n",
            "341 -> ac\n",
            "107 -> k\n",
            "256 ->  \n",
            "407 -> em\n",
            "98 -> b\n",
            "477 -> ra\n",
            "99 -> c\n",
            "385 -> ed\n",
            "256 ->  \n",
            "364 -> be\n",
            "97 -> a\n",
            "117 -> u\n",
            "116 -> t\n",
            "121 -> y\n",
            "284 ->  th\n",
            "456 -> ro\n",
            "117 -> u\n",
            "330 -> gh\n",
            "281 ->  a\n",
            "114 -> r\n",
            "116 -> t\n",
            "281 ->  a\n",
            "110 -> n\n",
            "100 -> d\n",
            "256 ->  \n",
            "326 -> li\n",
            "102 -> f\n",
            "101 -> e\n",
            "46 -> .\n"
          ]
        }
      ],
      "source": [
        "for id in token_ids:\n",
        "  print(id,'->',tokenizer1.decode([id]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sxYrr-lrJJ0",
        "outputId": "ac66ad58-9e5e-4b21-8ee1-cebea9f76b44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[84, 278, 115, 256, 300, 322, 306, 101, 260, 101, 120, 116, 46]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer1.encode(\"This is some text.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "Laa6xFYSGGvY",
        "outputId": "aa97cf55-2ae9-4ee0-9e73-4e60e6ca15ab"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "No token IDs found for the characters:['ജ']",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4aeec16aaf94>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ജis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-4de5ae064b5d>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtoken_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mtoken_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_using_bpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4de5ae064b5d>\u001b[0m in \u001b[0;36mtokenize_using_bpe\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m           \u001b[0mmissing_characters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No token IDs found for the characters:{missing_characters}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Step 3: Now merging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No token IDs found for the characters:['ജ']"
          ]
        }
      ],
      "source": [
        "# Unknown token\n",
        "tokenizer1.encode('ജis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "7Siae0mdF87h",
        "outputId": "10b2be59-ed51-4a60-a6bb-1f63b616c014"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "No token found for the token IDs:[5000]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3f5067297d61>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m542\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m321\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m305\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m259\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m461\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m116\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-4de5ae064b5d>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mnon_existing_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_existing_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No token found for the token IDs:{non_existing_ids}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Step 2: Decoding- Check whether text corresponding to token ID starts with word_split-symbol('Ġ'). If yes replace word_split-symbol with \" \" else just append the text to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No token found for the token IDs:[5000]"
          ]
        }
      ],
      "source": [
        "# Unknown token ID\n",
        "tokenizer1.decode([542, 299, 256, 299, 321, 305, 101, 259, 461, 116, 5000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LJeoJHQprBfW",
        "outputId": "331a22fe-e008-476a-9d37-98fe82591b04"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is some text.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Decoder\n",
        "tokenizer1.decode(tokenizer1.encode(\"This is some text.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "lHMNQ3tZwTSV"
      },
      "outputs": [],
      "source": [
        "tokenizer1.save_bpe_vocab_and_merges('train_vocab.json','train_bpe.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-z_dTzhQxOU5"
      },
      "outputs": [],
      "source": [
        "tokenizer2=BytePairEncodingTokenizer()\n",
        "tokenizer2.load_bpe_vocab_and_merges('train_vocab.json','train_bpe.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo9_SzQcxSud",
        "outputId": "88514267-28da-478a-83fd-74e859b8cde6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[363, 338, 256, 338, 285, 448, 263, 361, 116, 46]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer2.encode(\"This is some text.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0ufud6UWxb3y",
        "outputId": "227d87dd-48df-45c4-c230-aae5e1088f78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'This is some text.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer2.decode(tokenizer2.encode(\"This is some text.\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "mlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
