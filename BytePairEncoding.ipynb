{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[BPE Blog](https://sebastianraschka.com/blog/2025/bpe-from-scratch.html)\n",
        "\n",
        "[Research Paper](https://aclanthology.org/P16-1162.pdf)"
      ],
      "metadata": {
        "id": "6bUCse4WG-gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import json"
      ],
      "metadata": {
        "id": "CpRLnLkZf8ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKvHz5tB_3Fz"
      },
      "outputs": [],
      "source": [
        "class BytePairEncodingTokenizer:\n",
        "  def __init__(self,word_split = 'Ġ'):\n",
        "    self.vocab={}\n",
        "    self.inverse_vocab={}\n",
        "    self.tokens=[]\n",
        "    self.bpe_pairs={}\n",
        "    self.word_split = word_split\n",
        "\n",
        "  def set_vocabulary(self,word_split,special_tokens):\n",
        "    # Step 1: Setting the unique characters\n",
        "    unique_characters=[chr(i) for i in range(256)]\n",
        "\n",
        "    # Step 2: Word split characters\n",
        "    if word_split not in unique_characters:\n",
        "      unique_characters.append(self.word_split)\n",
        "\n",
        "    # Step 3: Special tokens\n",
        "    if special_tokens:\n",
        "      unique_characters.extend(self.special_tokens)\n",
        "\n",
        "    return unique_characters\n",
        "\n",
        "  def get_max_freq_pair(self,tokens):\n",
        "    pair_counts={}\n",
        "    pairs=[]\n",
        "    # Step 1: Gettkng all the token pairs-> (token[i],token[i+1])\n",
        "    for index in range(len(tokens)-1):\n",
        "      pairs.append((tokens[index],tokens[index+1]))\n",
        "\n",
        "    # Step 2: Getting the count of occurences of each of token pairs\n",
        "    pairs_counts=Counter(pairs)\n",
        "\n",
        "    # Step 3: Get the token pair whose count is the highest\n",
        "    max_pair=max(pairs_counts.items(),key=lambda x: x[1])[0]\n",
        "    return max_pair\n",
        "\n",
        "  def merge_tokens(self,tokens,max_pair,new_pair_id):\n",
        "    # In the tokens, check the presence of occurence of max_pair and if exists then replace max_pair with new_pair_id\n",
        "    # Eg: tokens=[87,76,44,25,38,44,25,19], max_pair=[44,25],  new_pair_id=123\n",
        "    # Output: [87,76,123,38,123,19]\n",
        "    new_tokens=[]\n",
        "    i=0\n",
        "    while i<=len(tokens)-1:\n",
        "      if i==len(tokens)-1:\n",
        "        new_tokens.append(tokens[i])\n",
        "        break\n",
        "\n",
        "      elif (tokens[i],tokens[i+1])==max_pair:\n",
        "        new_tokens.append(new_pair_id)\n",
        "        i+=2\n",
        "\n",
        "      else:\n",
        "        new_tokens.append(tokens[i])\n",
        "        i+=1\n",
        "    return new_tokens\n",
        "\n",
        "  def train(self,text,vocab_size,special_tokens):\n",
        "    if vocab_size<=258:\n",
        "      raise ValueError('Please enter a vocab size greater than 258 since this defines the basic set of characters')\n",
        "    self.special_tokens = special_tokens\n",
        "\n",
        "    # Setting the vocabulary\n",
        "    vocab = self.set_vocabulary(self.word_split,self.special_tokens)\n",
        "    for index,character in enumerate(vocab):\n",
        "      self.vocab[index]=character\n",
        "      self.inverse_vocab[character]=index\n",
        "\n",
        "    # Transforming the text\n",
        "    ## Step 1: Replacing all thw white-space character\n",
        "    processed_text=[]\n",
        "    for index,char in enumerate(text):\n",
        "      if index!=0 and char==' ':\n",
        "        processed_text.append(self.word_split)\n",
        "      if char!=' ':\n",
        "        processed_text.append(char)\n",
        "    processed_text=\"\".join(processed_text)\n",
        "\n",
        "    ## Step 2: Getting the numerical form of token\n",
        "    self.tokens=[]\n",
        "    for char in processed_text:\n",
        "      self.tokens.append(self.inverse_vocab[char])\n",
        "\n",
        "    ## Step 3: BPE-algorithm\n",
        "    vocab_length=len(self.vocab)\n",
        "    for i in range(vocab_length,vocab_size):\n",
        "      max_pair=self.get_max_freq_pair(self.tokens)\n",
        "      if max_pair is None:\n",
        "        break\n",
        "      self.bpe_pairs[max_pair]=i\n",
        "      self.tokens=self.merge_tokens(self.tokens,max_pair,i)\n",
        "\n",
        "    ## Step 4: Update vocab with BPE\n",
        "    for pair,new_index in self.bpe_pairs.items():\n",
        "      merged_token=self.vocab[pair[0]]+self.vocab[pair[1]]\n",
        "      self.vocab[new_index]=merged_token\n",
        "      self.inverse_vocab[merged_token]=new_index\n",
        "\n",
        "  def encode(self,text):\n",
        "    # Step 1: Basically tokens are split into words. Replace all the occurences of \"\\n\" to \" <NEWLINE> \". This is to avoid splitting issues.\n",
        "    tokens_split=text.replace('\\n',' <NEWLINE> ').split()\n",
        "    tokens=[]\n",
        "    for i in tokens_split:\n",
        "      if i=='<NEWLINE>':\n",
        "        tokens.append('\\n')\n",
        "      else:\n",
        "        tokens.append(i)\n",
        "\n",
        "    # Step 2: Cleaning of tokens\n",
        "    ## Eg: 'This is a ball' will be tokenized as ['The','Ġis','Ġa', 'Ġball']\n",
        "    # Ensures that all the tokens in a line other than the first one will be prefixed with \"Ġ\" to show the word boundaries\n",
        "    tokens_cleaned=[]\n",
        "    for index,token in enumerate(tokens):\n",
        "      if index>0 and not token.startswith('\\n'):\n",
        "        tokens_cleaned.append(self.word_split+token)\n",
        "      else:\n",
        "        tokens_cleaned.append(token)\n",
        "\n",
        "    # Step 3: Getting the corresponding token IDs from the cleaned tokens\n",
        "    ## Checks whether tokens exist in the vocabulary. If not, then perform BPE tokenization of the token\n",
        "    token_ids=[]\n",
        "    for token in tokens_cleaned:\n",
        "      if token in self.inverse_vocab.keys():\n",
        "        token_ids.append(self.inverse_vocab[token])\n",
        "      else:\n",
        "        token_ids.extend(self.tokenize_using_bpe(token))\n",
        "    return token_ids\n",
        "\n",
        "  def tokenize_using_bpe(self,token):\n",
        "    # Step 1: Mapping the tokens to their IDs from the vocabulary\n",
        "    token_ids=[]\n",
        "    for char in token:\n",
        "      if char in self.inverse_vocab.keys():\n",
        "        token_ids.append(self.inverse_vocab[char])\n",
        "      else:\n",
        "        token_ids.append(None)\n",
        "\n",
        "    # Step 2: Check whether token does not exist in Vocabulary- In that case stop\n",
        "    if None in token_ids:\n",
        "      token_dict=dict(zip(token_ids,token))\n",
        "      missing_characters=[]\n",
        "      for id,ch in token_dict.items():\n",
        "        if id is None:\n",
        "          missing_characters.append(ch)\n",
        "      raise ValueError(f\"No token IDs found for the characters:{missing_characters}\")\n",
        "\n",
        "    # Step 3: Now merging\n",
        "    can_merge=True\n",
        "    while can_merge and len(token_ids)>1:\n",
        "      can_merge=False\n",
        "      i=0\n",
        "      new_tokens=[]\n",
        "      \"\"\"\n",
        "      Check whether the token pair is part of bpe_pairs occured during training,\n",
        "      If yes, index = index + 2, else index = index + 1.\n",
        "      This iteration occurs until there exists no merging exists for all the tokens in token_ids.\n",
        "      No merging exists means that there are no more possible keys to merge in bpe_pairs.\n",
        "      \"\"\"\n",
        "      while i<len(token_ids)-1:\n",
        "        pair=(token_ids[i],token_ids[i+1])\n",
        "        if pair in self.bpe_pairs.keys():\n",
        "          pair_id=self.bpe_pairs[pair]\n",
        "          new_tokens.append(pair_id)\n",
        "          i+=2\n",
        "          can_merge=True\n",
        "        else:\n",
        "          new_tokens.append(token_ids[i])\n",
        "          i+=1\n",
        "      if i<len(token_ids):\n",
        "        new_tokens.append(token_ids[i])\n",
        "      token_ids=new_tokens\n",
        "\n",
        "    return token_ids\n",
        "\n",
        "  def decode(self,token_ids):\n",
        "    # Step 1: Check whether there are non-existing token IDs\n",
        "    non_existing_ids=[]\n",
        "    for id in token_ids:\n",
        "      if id not in self.vocab.keys():\n",
        "        non_existing_ids.append(id)\n",
        "    if len(non_existing_ids)>0:\n",
        "      raise ValueError(f\"No token found for the token IDs:{non_existing_ids}\")\n",
        "\n",
        "    # Step 2: Decoding- Check whether text corresponding to token ID starts with word_split-symbol('Ġ'). If yes replace word_split-symbol with \" \" else just append the text to string\n",
        "    final=\"\"\n",
        "    for id in token_ids:\n",
        "      text=self.vocab[id]\n",
        "      if text.startswith(self.word_split):\n",
        "        final+=\" \"+text[1:]\n",
        "      else:\n",
        "        final+=\"\"+text\n",
        "\n",
        "    return final\n",
        "\n",
        "  def save_bpe_vocab_and_merges(self,vocab_path,bpe_path):\n",
        "    with open(vocab_path,'w',encoding='utf-8') as f:\n",
        "      json.dump(self.vocab,f,ensure_ascii=False, indent=2)\n",
        "    with open(bpe_path,'w',encoding='utf-8') as f:\n",
        "      json.dump([{'pair':list(pair),'id':id } for pair,id in self.bpe_pairs.items()],f,\n",
        "                ensure_ascii=False, indent=2)\n",
        "\n",
        "  def load_bpe_vocab_and_merges(self,vocab_path,bpe_path):\n",
        "    with open(vocab_path,'r',encoding='utf-8') as f:\n",
        "      loaded_vocab=json.load(f)\n",
        "      self.vocab = {int(id):token for id,token in loaded_vocab.items()}\n",
        "      self.inverse_vocab={token:int(id) for id,token in self.vocab.items()}\n",
        "    with open(bpe_path,'r',encoding='utf-8') as f:\n",
        "      bpe=json.load(f)\n",
        "      for merge in bpe:\n",
        "        self.bpe_pairs[tuple(merge['pair'])]=merge['id']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "eMdYBakygQKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1 = BytePairEncodingTokenizer()\n",
        "tokenizer1.train(text, vocab_size=259, special_tokens={\"<|endoftext|>\",\"œ\"})"
      ],
      "metadata": {
        "id": "_Kdno3cSgSve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1.vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq6H_qJ59Tul",
        "outputId": "70be5d04-a89c-496f-ef97-7caa51d48b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\x00',\n",
              " 1: '\\x01',\n",
              " 2: '\\x02',\n",
              " 3: '\\x03',\n",
              " 4: '\\x04',\n",
              " 5: '\\x05',\n",
              " 6: '\\x06',\n",
              " 7: '\\x07',\n",
              " 8: '\\x08',\n",
              " 9: '\\t',\n",
              " 10: '\\n',\n",
              " 11: '\\x0b',\n",
              " 12: '\\x0c',\n",
              " 13: '\\r',\n",
              " 14: '\\x0e',\n",
              " 15: '\\x0f',\n",
              " 16: '\\x10',\n",
              " 17: '\\x11',\n",
              " 18: '\\x12',\n",
              " 19: '\\x13',\n",
              " 20: '\\x14',\n",
              " 21: '\\x15',\n",
              " 22: '\\x16',\n",
              " 23: '\\x17',\n",
              " 24: '\\x18',\n",
              " 25: '\\x19',\n",
              " 26: '\\x1a',\n",
              " 27: '\\x1b',\n",
              " 28: '\\x1c',\n",
              " 29: '\\x1d',\n",
              " 30: '\\x1e',\n",
              " 31: '\\x1f',\n",
              " 32: ' ',\n",
              " 33: '!',\n",
              " 34: '\"',\n",
              " 35: '#',\n",
              " 36: '$',\n",
              " 37: '%',\n",
              " 38: '&',\n",
              " 39: \"'\",\n",
              " 40: '(',\n",
              " 41: ')',\n",
              " 42: '*',\n",
              " 43: '+',\n",
              " 44: ',',\n",
              " 45: '-',\n",
              " 46: '.',\n",
              " 47: '/',\n",
              " 48: '0',\n",
              " 49: '1',\n",
              " 50: '2',\n",
              " 51: '3',\n",
              " 52: '4',\n",
              " 53: '5',\n",
              " 54: '6',\n",
              " 55: '7',\n",
              " 56: '8',\n",
              " 57: '9',\n",
              " 58: ':',\n",
              " 59: ';',\n",
              " 60: '<',\n",
              " 61: '=',\n",
              " 62: '>',\n",
              " 63: '?',\n",
              " 64: '@',\n",
              " 65: 'A',\n",
              " 66: 'B',\n",
              " 67: 'C',\n",
              " 68: 'D',\n",
              " 69: 'E',\n",
              " 70: 'F',\n",
              " 71: 'G',\n",
              " 72: 'H',\n",
              " 73: 'I',\n",
              " 74: 'J',\n",
              " 75: 'K',\n",
              " 76: 'L',\n",
              " 77: 'M',\n",
              " 78: 'N',\n",
              " 79: 'O',\n",
              " 80: 'P',\n",
              " 81: 'Q',\n",
              " 82: 'R',\n",
              " 83: 'S',\n",
              " 84: 'T',\n",
              " 85: 'U',\n",
              " 86: 'V',\n",
              " 87: 'W',\n",
              " 88: 'X',\n",
              " 89: 'Y',\n",
              " 90: 'Z',\n",
              " 91: '[',\n",
              " 92: '\\\\',\n",
              " 93: ']',\n",
              " 94: '^',\n",
              " 95: '_',\n",
              " 96: '`',\n",
              " 97: 'a',\n",
              " 98: 'b',\n",
              " 99: 'c',\n",
              " 100: 'd',\n",
              " 101: 'e',\n",
              " 102: 'f',\n",
              " 103: 'g',\n",
              " 104: 'h',\n",
              " 105: 'i',\n",
              " 106: 'j',\n",
              " 107: 'k',\n",
              " 108: 'l',\n",
              " 109: 'm',\n",
              " 110: 'n',\n",
              " 111: 'o',\n",
              " 112: 'p',\n",
              " 113: 'q',\n",
              " 114: 'r',\n",
              " 115: 's',\n",
              " 116: 't',\n",
              " 117: 'u',\n",
              " 118: 'v',\n",
              " 119: 'w',\n",
              " 120: 'x',\n",
              " 121: 'y',\n",
              " 122: 'z',\n",
              " 123: '{',\n",
              " 124: '|',\n",
              " 125: '}',\n",
              " 126: '~',\n",
              " 127: '\\x7f',\n",
              " 128: '\\x80',\n",
              " 129: '\\x81',\n",
              " 130: '\\x82',\n",
              " 131: '\\x83',\n",
              " 132: '\\x84',\n",
              " 133: '\\x85',\n",
              " 134: '\\x86',\n",
              " 135: '\\x87',\n",
              " 136: '\\x88',\n",
              " 137: '\\x89',\n",
              " 138: '\\x8a',\n",
              " 139: '\\x8b',\n",
              " 140: '\\x8c',\n",
              " 141: '\\x8d',\n",
              " 142: '\\x8e',\n",
              " 143: '\\x8f',\n",
              " 144: '\\x90',\n",
              " 145: '\\x91',\n",
              " 146: '\\x92',\n",
              " 147: '\\x93',\n",
              " 148: '\\x94',\n",
              " 149: '\\x95',\n",
              " 150: '\\x96',\n",
              " 151: '\\x97',\n",
              " 152: '\\x98',\n",
              " 153: '\\x99',\n",
              " 154: '\\x9a',\n",
              " 155: '\\x9b',\n",
              " 156: '\\x9c',\n",
              " 157: '\\x9d',\n",
              " 158: '\\x9e',\n",
              " 159: '\\x9f',\n",
              " 160: '\\xa0',\n",
              " 161: '¡',\n",
              " 162: '¢',\n",
              " 163: '£',\n",
              " 164: '¤',\n",
              " 165: '¥',\n",
              " 166: '¦',\n",
              " 167: '§',\n",
              " 168: '¨',\n",
              " 169: '©',\n",
              " 170: 'ª',\n",
              " 171: '«',\n",
              " 172: '¬',\n",
              " 173: '\\xad',\n",
              " 174: '®',\n",
              " 175: '¯',\n",
              " 176: '°',\n",
              " 177: '±',\n",
              " 178: '²',\n",
              " 179: '³',\n",
              " 180: '´',\n",
              " 181: 'µ',\n",
              " 182: '¶',\n",
              " 183: '·',\n",
              " 184: '¸',\n",
              " 185: '¹',\n",
              " 186: 'º',\n",
              " 187: '»',\n",
              " 188: '¼',\n",
              " 189: '½',\n",
              " 190: '¾',\n",
              " 191: '¿',\n",
              " 192: 'À',\n",
              " 193: 'Á',\n",
              " 194: 'Â',\n",
              " 195: 'Ã',\n",
              " 196: 'Ä',\n",
              " 197: 'Å',\n",
              " 198: 'Æ',\n",
              " 199: 'Ç',\n",
              " 200: 'È',\n",
              " 201: 'É',\n",
              " 202: 'Ê',\n",
              " 203: 'Ë',\n",
              " 204: 'Ì',\n",
              " 205: 'Í',\n",
              " 206: 'Î',\n",
              " 207: 'Ï',\n",
              " 208: 'Ð',\n",
              " 209: 'Ñ',\n",
              " 210: 'Ò',\n",
              " 211: 'Ó',\n",
              " 212: 'Ô',\n",
              " 213: 'Õ',\n",
              " 214: 'Ö',\n",
              " 215: '×',\n",
              " 216: 'Ø',\n",
              " 217: 'Ù',\n",
              " 218: 'Ú',\n",
              " 219: 'Û',\n",
              " 220: 'Ü',\n",
              " 221: 'Ý',\n",
              " 222: 'Þ',\n",
              " 223: 'ß',\n",
              " 224: 'à',\n",
              " 225: 'á',\n",
              " 226: 'â',\n",
              " 227: 'ã',\n",
              " 228: 'ä',\n",
              " 229: 'å',\n",
              " 230: 'æ',\n",
              " 231: 'ç',\n",
              " 232: 'è',\n",
              " 233: 'é',\n",
              " 234: 'ê',\n",
              " 235: 'ë',\n",
              " 236: 'ì',\n",
              " 237: 'í',\n",
              " 238: 'î',\n",
              " 239: 'ï',\n",
              " 240: 'ð',\n",
              " 241: 'ñ',\n",
              " 242: 'ò',\n",
              " 243: 'ó',\n",
              " 244: 'ô',\n",
              " 245: 'õ',\n",
              " 246: 'ö',\n",
              " 247: '÷',\n",
              " 248: 'ø',\n",
              " 249: 'ù',\n",
              " 250: 'ú',\n",
              " 251: 'û',\n",
              " 252: 'ü',\n",
              " 253: 'ý',\n",
              " 254: 'þ',\n",
              " 255: 'ÿ',\n",
              " 256: 'Ġ',\n",
              " 257: '<|endoftext|>',\n",
              " 258: 'œ'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer1.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCwxtqB1gtkr",
        "outputId": "1d73ccc9-3b07-4674-9272-844f8433f423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer1.bpe_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQnpoYJSgvk6",
        "outputId": "b7939144-4591-4dd1-d36c-636c09c5844f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Jack embraced beauty through art and life.\"\n",
        "token_ids = tokenizer1.encode(input_text)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsf1QSMfg1Qr",
        "outputId": "1a4bc556-a305-4d0a-c909-f2c4b29260d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[425, 256, 101, 109, 98, 334, 99, 312, 256, 297, 97, 117, 116, 121, 304, 336, 117, 307, 288, 114, 116, 288, 110, 100, 256, 327, 102, 101, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer1.decode(token_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI5FK_DCqbjP",
        "outputId": "af2ec113-54f5-49fe-a76c-4137ad866667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jack embraced beauty through art and life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for id in token_ids:\n",
        "  print(id,'->',tokenizer1.decode([id]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxLp4CTvqkR4",
        "outputId": "514db531-acdf-44f9-da91-7d3b5e79a0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "425 -> Jack\n",
            "256 ->  \n",
            "101 -> e\n",
            "109 -> m\n",
            "98 -> b\n",
            "334 -> ra\n",
            "99 -> c\n",
            "312 -> ed\n",
            "256 ->  \n",
            "297 -> be\n",
            "97 -> a\n",
            "117 -> u\n",
            "116 -> t\n",
            "121 -> y\n",
            "304 ->  th\n",
            "336 -> ro\n",
            "117 -> u\n",
            "307 -> gh\n",
            "288 ->  a\n",
            "114 -> r\n",
            "116 -> t\n",
            "288 ->  a\n",
            "110 -> n\n",
            "100 -> d\n",
            "256 ->  \n",
            "327 -> li\n",
            "102 -> f\n",
            "101 -> e\n",
            "46 -> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1.encode(\"This is some text.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sxYrr-lrJJ0",
        "outputId": "ac66ad58-9e5e-4b21-8ee1-cebea9f76b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[84, 278, 115, 256, 300, 322, 306, 101, 260, 101, 120, 116, 46]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1.encode('ജis')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "Laa6xFYSGGvY",
        "outputId": "aa97cf55-2ae9-4ee0-9e73-4e60e6ca15ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No token IDs found for the characters:['ജ']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4aeec16aaf94>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ജis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-4de5ae064b5d>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtoken_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mtoken_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_using_bpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4de5ae064b5d>\u001b[0m in \u001b[0;36mtokenize_using_bpe\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m           \u001b[0mmissing_characters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No token IDs found for the characters:{missing_characters}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Step 3: Now merging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No token IDs found for the characters:['ജ']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1.decode([542, 299, 256, 299, 321, 305, 101, 259, 461, 116, 5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "7Siae0mdF87h",
        "outputId": "10b2be59-ed51-4a60-a6bb-1f63b616c014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No token found for the token IDs:[5000]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3f5067297d61>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m542\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m321\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m305\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m259\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m461\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m116\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-4de5ae064b5d>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mnon_existing_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_existing_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No token found for the token IDs:{non_existing_ids}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Step 2: Decoding- Check whether text corresponding to token ID starts with word_split-symbol('Ġ'). If yes replace word_split-symbol with \" \" else just append the text to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No token found for the token IDs:[5000]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1.decode(tokenizer1.encode(\"This is some text.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LJeoJHQprBfW",
        "outputId": "331a22fe-e008-476a-9d37-98fe82591b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is some text.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1.save_bpe_vocab_and_merges('vocab.json','bpe.json')"
      ],
      "metadata": {
        "id": "lHMNQ3tZwTSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2=BytePairEncodingTokenizer()\n",
        "tokenizer2.load_bpe_vocab_and_merges('vocab.json','bpe.json')"
      ],
      "metadata": {
        "id": "-z_dTzhQxOU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2.encode(\"This is some text.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo9_SzQcxSud",
        "outputId": "88514267-28da-478a-83fd-74e859b8cde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[542, 299, 256, 299, 321, 305, 101, 259, 461, 116, 46]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2.decode([542, 299, 256, 299, 321, 305, 101, 259, 461, 116, 46])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0ufud6UWxb3y",
        "outputId": "227d87dd-48df-45c4-c230-aae5e1088f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is some text.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer2.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OleLPgmXyMz4",
        "outputId": "dc37a6f1-1e68-40e8-a6f7-7cc25d585a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer2.bpe_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr4n61k1yONW",
        "outputId": "bf84c64f-86c9-4809-cd73-53979999554d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "742"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}