{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7626205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9194f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_kq, d_v):\n",
    "        super().__init__()\n",
    "        self.d_kq = d_kq\n",
    "        self.q_weights = nn.Parameter(torch.rand(d_in, d_kq))\n",
    "        self.k_weights = nn.Parameter(torch.rand(d_in, d_kq))\n",
    "        self.v_weights = nn.Parameter(torch.rand(d_in, d_v))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        query = x @ self.q_weights\n",
    "        key = x @ self.k_weights\n",
    "        value = x @ self.v_weights\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.zeros((x.shape[-2], x.shape[-2]))\n",
    "\n",
    "        attn_scores = query @ torch.transpose(key, -1, -2)\n",
    "        masked_attn_scores = attn_scores.masked_fill(attention_mask.bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(masked_attn_scores / self.d_kq**0.5, dim=-1)\n",
    "        return attn_weights @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a0e9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_kq, d_v):\n",
    "        super().__init__()\n",
    "        self.d_kq = d_kq\n",
    "        self.q_weights = nn.Parameter(torch.rand(d_in, d_kq))\n",
    "        self.k_weights = nn.Parameter(torch.rand(d_in, d_kq))\n",
    "        self.v_weights = nn.Parameter(torch.rand(d_in, d_v))\n",
    "\n",
    "    def forward(self, x, encoder_x, attention_mask=None):\n",
    "        query = x @ self.q_weights\n",
    "        key = encoder_x @ self.k_weights\n",
    "        value = encoder_x @ self.v_weights\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.zeros((x.shape[-2], encoder_x.shape[-2]))\n",
    "\n",
    "        attn_scores = query @ torch.transpose(key, -1, -2)\n",
    "        masked_attn_scores = attn_scores.masked_fill(attention_mask.bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(masked_attn_scores / self.d_kq**0.5, dim=-1)\n",
    "        return attn_weights @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8878806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_kq, d_v, num_heads, attn_type):\n",
    "        super().__init__()\n",
    "        self.attn_type = attn_type\n",
    "        if attn_type == \"self\":\n",
    "            self.heads = nn.ModuleList([SelfAttention(d_in, d_kq, d_v) for _ in range(num_heads)])\n",
    "        elif attn_type == \"cross\":\n",
    "            self.heads = nn.ModuleList([CrossAttention(d_in, d_kq, d_v) for _ in range(num_heads)])\n",
    "        else:\n",
    "            raise ValueError(\"attn_type should be either 'self' or 'cross'.\")\n",
    "\n",
    "    def forward(self, x, encoder_x=None, attention_mask=None):\n",
    "        if self.attn_type == \"self\":\n",
    "            return torch.cat([head(x, attention_mask) for head in self.heads], dim=-1)\n",
    "        else:\n",
    "            assert encoder_x is not None\n",
    "            return torch.cat([head(x, encoder_x, attention_mask) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "271a935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((3, 10, 4))\n",
    "num_heads = 4 # should be a factor of the input embedding dimension\n",
    "d_in = x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "129ea38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1020, 1.2812, 1.1678, 0.6591],\n",
       "         [1.1390, 1.2630, 1.1815, 0.6455],\n",
       "         [1.1505, 1.2572, 1.2308, 0.6521],\n",
       "         [1.0164, 1.1798, 1.0717, 0.6096],\n",
       "         [1.1310, 1.2302, 1.1732, 0.6401],\n",
       "         [0.9395, 1.1880, 1.0003, 0.5949],\n",
       "         [1.0072, 1.1757, 1.0807, 0.5926],\n",
       "         [1.1049, 1.2723, 1.1463, 0.6492],\n",
       "         [0.9459, 1.1627, 1.0231, 0.5941],\n",
       "         [1.0344, 1.2051, 1.0586, 0.6200]],\n",
       "\n",
       "        [[1.0565, 1.2376, 1.1092, 0.6287],\n",
       "         [1.0131, 1.1884, 1.0658, 0.5924],\n",
       "         [1.0106, 1.1686, 1.0631, 0.5924],\n",
       "         [1.0133, 1.1804, 1.0842, 0.5865],\n",
       "         [1.1307, 1.2880, 1.2340, 0.6702],\n",
       "         [0.9289, 1.1238, 0.9584, 0.5365],\n",
       "         [1.0138, 1.1989, 1.1022, 0.5883],\n",
       "         [1.0265, 1.2034, 1.0854, 0.5961],\n",
       "         [0.9708, 1.1396, 1.0344, 0.5606],\n",
       "         [1.0020, 1.1658, 1.0357, 0.5864]],\n",
       "\n",
       "        [[1.3801, 1.5805, 1.5027, 0.7440],\n",
       "         [1.3828, 1.5768, 1.5108, 0.7442],\n",
       "         [1.3772, 1.5872, 1.4833, 0.7425],\n",
       "         [1.3837, 1.5642, 1.4912, 0.7307],\n",
       "         [1.4411, 1.5998, 1.5833, 0.7630],\n",
       "         [1.3820, 1.5863, 1.5101, 0.7491],\n",
       "         [1.3192, 1.5245, 1.4371, 0.7003],\n",
       "         [1.3669, 1.5756, 1.4866, 0.7351],\n",
       "         [1.3816, 1.5728, 1.5230, 0.7372],\n",
       "         [1.4053, 1.5693, 1.5333, 0.7411]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = MultiHeadAttention(d_in=d_in, d_kq = 2, d_v=d_in//num_heads, num_heads=num_heads, attn_type=\"self\")\n",
    "attn_out = attn(x)\n",
    "attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93eb4644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 4])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ac6696c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masked self attention\n",
    "attn_mask = torch.ones((x.shape[-2], x.shape[-2])) # mask everything out\n",
    "masked_attn_out = attn(x, attention_mask=attn_mask)\n",
    "masked_attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f97f687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.3479, 1.4108, 1.5758, 0.5809],\n",
       "         [1.1998, 1.5331, 1.2518, 0.7061],\n",
       "         [1.3952, 1.6170, 1.3963, 0.7079],\n",
       "         [1.2700, 1.4358, 1.2939, 0.6728],\n",
       "         [1.3060, 1.4643, 1.3055, 0.7131],\n",
       "         [1.0887, 1.3328, 1.1396, 0.6430],\n",
       "         [1.0930, 1.2644, 1.1313, 0.6113],\n",
       "         [1.1566, 1.3594, 1.1821, 0.6647],\n",
       "         [0.9795, 1.1972, 1.0469, 0.5930],\n",
       "         [1.0344, 1.2051, 1.0586, 0.6200]],\n",
       "\n",
       "        [[0.9822, 1.3813, 1.1298, 0.6932],\n",
       "         [0.9153, 1.2364, 1.0199, 0.6251],\n",
       "         [0.9466, 1.1497, 1.0459, 0.6128],\n",
       "         [0.9421, 1.1454, 0.9991, 0.5804],\n",
       "         [1.2732, 1.4739, 1.3936, 0.7595],\n",
       "         [0.9869, 1.1869, 1.0523, 0.5858],\n",
       "         [1.0773, 1.2857, 1.1756, 0.6160],\n",
       "         [1.0588, 1.2822, 1.1167, 0.6142],\n",
       "         [0.9847, 1.1697, 1.0418, 0.5593],\n",
       "         [1.0020, 1.1658, 1.0357, 0.5864]],\n",
       "\n",
       "        [[1.3049, 1.4533, 1.4994, 0.6550],\n",
       "         [1.3706, 1.4679, 1.5526, 0.6610],\n",
       "         [1.2700, 1.4445, 1.4605, 0.6651],\n",
       "         [1.2136, 1.4410, 1.3589, 0.6936],\n",
       "         [1.5269, 1.6938, 1.6901, 0.8343],\n",
       "         [1.4534, 1.6547, 1.6202, 0.7990],\n",
       "         [1.3247, 1.5100, 1.4740, 0.7126],\n",
       "         [1.3544, 1.5542, 1.5083, 0.7382],\n",
       "         [1.3782, 1.5548, 1.5341, 0.7227],\n",
       "         [1.4053, 1.5693, 1.5333, 0.7411]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# causal mask self attention\n",
    "attn_mask = torch.triu(torch.ones(x.shape[-2], x.shape[-2]), diagonal=1)\n",
    "masked_attn_out = attn(x, attention_mask=attn_mask)\n",
    "masked_attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac368209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0636, 1.6300, 0.9427, 1.1702],\n",
       "         [1.0666, 1.6138, 0.9201, 1.1986],\n",
       "         [1.0707, 1.6189, 0.9491, 1.2104],\n",
       "         [1.0348, 1.5422, 0.8956, 1.1456],\n",
       "         [1.0635, 1.5966, 0.9176, 1.1955],\n",
       "         [1.0049, 1.5263, 0.8846, 1.1035],\n",
       "         [1.0299, 1.5262, 0.8897, 1.1575],\n",
       "         [1.0605, 1.6176, 0.9208, 1.1739],\n",
       "         [1.0130, 1.5172, 0.8936, 1.1126],\n",
       "         [1.0369, 1.5615, 0.8887, 1.1417]],\n",
       "\n",
       "        [[1.0180, 1.6868, 1.0566, 1.1646],\n",
       "         [1.0207, 1.6812, 1.0537, 1.1679],\n",
       "         [1.0210, 1.6801, 1.0542, 1.1668],\n",
       "         [1.0205, 1.6802, 1.0546, 1.1686],\n",
       "         [1.0138, 1.6928, 1.0631, 1.1617],\n",
       "         [1.0275, 1.6719, 1.0459, 1.1747],\n",
       "         [1.0200, 1.6812, 1.0555, 1.1691],\n",
       "         [1.0198, 1.6825, 1.0543, 1.1683],\n",
       "         [1.0236, 1.6753, 1.0519, 1.1707],\n",
       "         [1.0219, 1.6795, 1.0521, 1.1678]],\n",
       "\n",
       "        [[1.1801, 2.0325, 1.3801, 1.4393],\n",
       "         [1.1801, 2.0337, 1.3822, 1.4408],\n",
       "         [1.1801, 2.0284, 1.3734, 1.4372],\n",
       "         [1.1797, 2.0227, 1.3679, 1.4457],\n",
       "         [1.1832, 2.0367, 1.3923, 1.4583],\n",
       "         [1.1805, 2.0358, 1.3844, 1.4383],\n",
       "         [1.1739, 2.0148, 1.3583, 1.4370],\n",
       "         [1.1794, 2.0286, 1.3750, 1.4388],\n",
       "         [1.1805, 2.0321, 1.3830, 1.4470],\n",
       "         [1.1810, 2.0295, 1.3796, 1.4516]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_attn = MultiHeadAttention(d_in=d_in, d_kq = 2, d_v=d_in//num_heads, num_heads=num_heads, attn_type=\"cross\")\n",
    "encoder_x = torch.rand((3, 5, 4))\n",
    "cross_attn_out = cross_attn(x, encoder_x)\n",
    "cross_attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37d3d98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 4])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_attn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349661e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
